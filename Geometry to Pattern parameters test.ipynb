{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducibility when testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x22f25f6ddb0>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see https://pytorch.org/docs/stable/notes/randomness.html\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# when using cuda\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import scipy.io  # for reading matlab matrix\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import openmesh as om"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = 'D:\\Data\\CLOTHING\\Learning Shared Shape Space_shirt_dataset_rest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom DataSet class\n",
    "class ParametrizedShirtDataSet(Dataset):\n",
    "    \"\"\"\n",
    "    For loading the data of \"Learning Shared Shape Space..\" paper\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir, transform = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the t-shirt examples as subfolders\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "        self.root_path = Path(root_dir)\n",
    "        \n",
    "        # list of items = subfolders\n",
    "        self.datapoints_names = next(os.walk(self.root_path))[1]\n",
    "        \n",
    "        # remove non-valid element\n",
    "        self.datapoints_names.remove('P3ORMPBNJJAJ')\n",
    "        \n",
    "        \n",
    "        # datapoint folder structure\n",
    "        self.mesh_filename = 'shirt_mesh_r.obj'\n",
    "        self.pattern_params_filename = 'shirt_info.txt'\n",
    "        self.features_filename = 'visfea.mat'\n",
    "        self.garment_3d_filename = 'shirt_mesh_r_tmp.obj'\n",
    "        \n",
    "    def update_transform(self, transform):\n",
    "        \"\"\"apply new transform when loading the data\"\"\"\n",
    "        self.transform = transform\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Number of entries in the dataset\"\"\"\n",
    "        return len(self.datapoints_names)\n",
    "    \n",
    "    \n",
    "    def read_verts(self, datapoint_name):\n",
    "        \"\"\"features parameters from a given datapoint subfolder\"\"\"\n",
    "        assert (self.root_path / datapoint_name / self.garment_3d_filename).exists(), datapoint_name\n",
    "        \n",
    "        mesh = om.read_trimesh(str(self.root_path / datapoint_name / self.garment_3d_filename))\n",
    "        \n",
    "        return mesh.points()\n",
    "    \n",
    "    \n",
    "    def read_pattern_params(self, datapoint_name):\n",
    "        \"\"\"9 pattern size parameters from a given datapoint subfolder\"\"\"\n",
    "        assert (self.root_path / datapoint_name / self.pattern_params_filename).exists(), datapoint_name\n",
    "        \n",
    "        # assuming that we need the numbers from the last line in file\n",
    "        with open(self.root_path / datapoint_name / self.pattern_params_filename) as f:\n",
    "            lines = f.readlines()\n",
    "            params = np.fromstring(lines[-1],  sep = ' ')\n",
    "        return params\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Called when indexing: read the corresponding data. \n",
    "        Does not support list indexing\"\"\"\n",
    "        \n",
    "        if torch.is_tensor(idx):  # allow indexing by tensors\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        datapoint_name = self.datapoints_names[idx]\n",
    "        \n",
    "        vert_list = self.read_verts(datapoint_name)\n",
    "        \n",
    "        # read the pattern parameters\n",
    "        pattern_parameters = self.read_pattern_params(datapoint_name)\n",
    "        \n",
    "        sample = {'features': vert_list.ravel(), 'pattern_params': pattern_parameters, 'name': datapoint_name}\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample\n",
    "        \n",
    "        \n",
    "    def save_prediction_batch(self, predicted_params, names):\n",
    "        \"\"\"Saves predicted params of the datapoint to the original data folder\"\"\"\n",
    "        \n",
    "        for prediction, name in zip(predicted_params, names):\n",
    "            path_to_prediction = self.root_path / '..' / 'predictions' / name\n",
    "            try:\n",
    "                os.makedirs(path_to_prediction)\n",
    "            except OSError:\n",
    "                pass\n",
    "            \n",
    "            prediction = prediction.tolist()\n",
    "            with open(path_to_prediction / self.pattern_params_filename, 'w+') as f:\n",
    "                f.writelines(['0\\n', '0\\n', ' '.join(map(str, prediction))])\n",
    "                print ('Saved ' + name)\n",
    "    \n",
    "    def feature_size(self):\n",
    "        return 12252 * 3\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transforms -- to tensor\n",
    "class SampleToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        features, params = sample['features'], sample['pattern_params']\n",
    "        \n",
    "        return {\n",
    "                'features': torch.from_numpy(features).float(), \n",
    "                'pattern_params': torch.from_numpy(params).float(), \n",
    "                'name': sample['name']\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transforms -- normalize\n",
    "class NormalizeInputfeatures(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "    def __init__(self, mean_features, std_features):\n",
    "        self.mean = mean_features\n",
    "        self.std = std_features\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        features = sample['features']\n",
    "        \n",
    "        return {\n",
    "                    'features': torch.div((features - self.mean), self.std), \n",
    "                    'pattern_params': sample['pattern_params'], \n",
    "                    'name': sample['name']\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Normalization?\n",
    "\n",
    "def get_mean_std(dataloader):\n",
    "    \n",
    "    stats = { 'batch_sums' : [], 'batch_sq_sums' : []}\n",
    "    \n",
    "    for data in dataloader:\n",
    "        batch_sum = data['features'].sum(0)\n",
    "        stats['batch_sums'].append(batch_sum)\n",
    "\n",
    "    mean_features = sum(stats['batch_sums']) / len(dataloader)\n",
    "    \n",
    "    for data in dataloader:\n",
    "        batch_sum_sq = (data['features'] - mean_features.view(1, len(mean_features)))**2\n",
    "        stats['batch_sq_sums'].append(batch_sum_sq.sum(0))\n",
    "                        \n",
    "    std_features = torch.sqrt(sum(stats['batch_sq_sums']) / len(dataloader))\n",
    "    \n",
    "    return mean_features, std_features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1049\n",
      "tensor([-2.6803, 12.6441, -0.1044,  ...,  2.7306, 12.6700, -0.0931])\n",
      "torch.Size([36756])\n",
      "torch.Size([9])\n"
     ]
    }
   ],
   "source": [
    "# test loading\n",
    "dataset = ParametrizedShirtDataSet(Path(data_location), \n",
    "                                  SampleToTensor())\n",
    "\n",
    "print (len(dataset))\n",
    "print (dataset[100]['features'])\n",
    "print (dataset[100]['features'].shape)\n",
    "print (dataset[0]['pattern_params'].shape)\n",
    "#print (dataset[1000])\n",
    "\n",
    "#loader = DataLoader(dataset, 10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test if all elements are avaliable\n",
    "\n",
    "\n",
    "for name in dataset.datapoints_names:\n",
    "    if not (dataset.root_path / name / dataset.garment_3d_filename).exists():\n",
    "        print (name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4847, 1.7808, 2.5189, 2.8437, 4.7678, 6.1286, 6.0374, 6.0461, 6.0504,\n",
      "        6.0500, 6.0500, 6.0500, 6.0501, 6.0495, 6.0338, 6.0290, 6.0191, 6.0103,\n",
      "        6.0033, 5.9944, 5.9856, 5.9771, 5.9658, 5.9569, 5.9481, 5.9396, 5.9291,\n",
      "        5.9197, 5.9068, 5.9005, 5.7688, 5.1725, 4.5954, 3.9547, 2.9973, 2.3022,\n",
      "        2.2025, 2.1716, 2.1283, 2.0894, 2.0596, 2.0363, 2.0191, 2.0048, 1.9907,\n",
      "        1.9804, 1.9743, 1.9572, 1.9458, 1.9341, 1.9225, 1.9199, 1.9182, 1.9112,\n",
      "        1.9119, 1.9027, 1.9047, 1.8997, 1.9000, 1.9002, 1.8983, 1.8905, 1.8855,\n",
      "        1.8806, 1.8821, 1.8798, 1.8794, 1.8862, 1.9059, 1.9212, 1.9365, 1.9526,\n",
      "        1.9691, 1.9886, 2.0051, 2.0236, 2.0428, 2.0614, 2.0781, 2.0829, 2.1120,\n",
      "        2.1466, 2.1775, 2.2130, 2.2419, 2.2715, 2.3000, 2.3252, 2.3484, 2.3717,\n",
      "        2.3947, 2.4142, 2.4290, 2.4452, 2.4613, 2.4773, 2.5043, 2.3297, 1.7479,\n",
      "        0.7207])\n",
      "tensor([-0.3243, -0.3104, -0.3113, -0.3173, -0.3096, -0.3099, -0.3147, -0.3157,\n",
      "        -0.3159, -0.3160, -0.3160, -0.3160, -0.3159, -0.3159, -0.3160, -0.3159,\n",
      "        -0.3159, -0.3159, -0.3158, -0.3157, -0.3155, -0.3153, -0.3149, -0.3144,\n",
      "        -0.3138, -0.3131, -0.3122, -0.3112, -0.3100, -0.3084, -0.3075, -0.3099,\n",
      "        -0.3123, -0.3155, -0.3221, -0.3272, -0.3264, -0.3248, -0.3236, -0.3226,\n",
      "        -0.3215, -0.3206, -0.3198, -0.3193, -0.3190, -0.3188, -0.3186, -0.3186,\n",
      "        -0.3186, -0.3187, -0.3188, -0.3188, -0.3187, -0.3188, -0.3187, -0.3188,\n",
      "        -0.3187, -0.3187, -0.3187, -0.3187, -0.3187, -0.3188, -0.3189, -0.3190,\n",
      "        -0.3190, -0.3190, -0.3191, -0.3190, -0.3188, -0.3187, -0.3186, -0.3185,\n",
      "        -0.3184, -0.3183, -0.3182, -0.3181, -0.3180, -0.3178, -0.3177, -0.3179,\n",
      "        -0.3176, -0.3172, -0.3169, -0.3165, -0.3162, -0.3159, -0.3158, -0.3157,\n",
      "        -0.3155, -0.3153, -0.3151, -0.3150, -0.3150, -0.3149, -0.3148, -0.3148,\n",
      "        -0.3143, -0.3154, -0.3190, -0.3238])\n"
     ]
    }
   ],
   "source": [
    "# Normalization of features\n",
    "mean, std = get_mean_std(loader)\n",
    "print (mean, std)\n",
    "\n",
    "dataset_normalized = ParametrizedShirtDataSet(Path(data_location), \n",
    "                                  transforms.Compose([\n",
    "                                      SampleToTensor(), \n",
    "                                      NormalizeInputfeatures(mean, std)]))\n",
    "\n",
    "print (dataset[1]['features'])\n",
    "print (dataset_normalized[1]['features'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShirtfeaturesMLP(nn.Module):\n",
    "    \"\"\"MLP for training on shirts dataset. Assumes 100 features parameters used\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # layers definitions\n",
    "        self.sequence = nn.Sequential(\n",
    "            nn.Linear(36756, 3000), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(3000, 300), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(300, 60),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(60, 9)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x_batch):\n",
    "        #print (x_batch)\n",
    "        \n",
    "        return self.sequence(x_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShirtfeaturesMLP(\n",
      "  (sequence): Sequential(\n",
      "    (0): Linear(in_features=36756, out_features=3000, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=3000, out_features=300, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=300, out_features=60, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=60, out_features=9, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = ShirtfeaturesMLP()\n",
    "\n",
    "print (net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Basic Parameters\n",
    "batch_size = 64\n",
    "epochs_num = 100\n",
    "learning_rate = 0.00001\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial load\n",
    "shirt_dataset = ParametrizedShirtDataSet(Path(data_location), \n",
    "                                  SampleToTensor())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data normalization\n",
    "mean, std = get_mean_std(DataLoader(shirt_dataset, 100))\n",
    "shirt_dataset = ParametrizedShirtDataSet(Path(data_location), \n",
    "                                  transforms.Compose([\n",
    "                                      SampleToTensor(), \n",
    "                                      NormalizeInputfeatures(mean, std)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split: 945 / 104\n"
     ]
    }
   ],
   "source": [
    "# Data load and split\n",
    "valid_size = (int) (len(shirt_dataset) / 10)\n",
    "# split is RANDOM. Might affect performance\n",
    "training_set, validation_set = torch.utils.data.random_split(\n",
    "    shirt_dataset, \n",
    "    (len(shirt_dataset) - valid_size, valid_size))\n",
    "\n",
    "print ('Split: {} / {}'.format(len(training_set), len(validation_set)))\n",
    "\n",
    "training_loader = DataLoader(training_set, batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_set, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop func\n",
    "\n",
    "def fit(model, regression_loss, optimizer, train_loader):\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range (epochs_num):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, batch in enumerate(training_loader):\n",
    "            features, params = batch['features'].to(device), batch['pattern_params'].to(device)\n",
    "            \n",
    "            #with torch.autograd.detect_anomaly():\n",
    "            preds = model(features)\n",
    "            loss = regression_loss(preds, params)\n",
    "            #print ('Epoch: {}, Batch: {}, Loss: {}'.format(epoch, i, loss))\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # logging\n",
    "            if i % 5 == 4:\n",
    "                wb.log({'epoch': epoch, 'loss': loss})\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(\n",
    "                *[(regression_loss(model(features), params), len(batch)) for batch in validation_loader]\n",
    "            )\n",
    "            \n",
    "        valid_loss = np.sum(losses) / np.sum(nums)\n",
    "        print ('Epoch: {}, Validation Loss: {}'.format(epoch, valid_loss))\n",
    "        wb.log({'epoch': epoch, 'valid_loss': valid_loss})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the definitions\n",
    "# model\n",
    "model = ShirtfeaturesMLP()\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "# loss function\n",
    "regression_loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2414, -0.0883,  0.0889,  0.6119,  0.0395,  0.7041,  0.4997,  0.1071,\n",
      "          0.1445],\n",
      "        [ 0.2458, -0.0847,  0.0844,  0.6072,  0.0382,  0.7028,  0.4876,  0.1052,\n",
      "          0.1425],\n",
      "        [ 0.2435, -0.0814,  0.0975,  0.6140,  0.0406,  0.6994,  0.4985,  0.1132,\n",
      "          0.1411],\n",
      "        [ 0.2485, -0.0854,  0.0849,  0.6087,  0.0388,  0.7054,  0.4902,  0.1085,\n",
      "          0.1438],\n",
      "        [ 0.2466, -0.0833,  0.0824,  0.6033,  0.0397,  0.7027,  0.4860,  0.1088,\n",
      "          0.1436],\n",
      "        [ 0.2420, -0.0882,  0.0969,  0.6158,  0.0396,  0.6982,  0.5048,  0.1130,\n",
      "          0.1381],\n",
      "        [ 0.2360, -0.0828,  0.0953,  0.6124,  0.0403,  0.6988,  0.4980,  0.1096,\n",
      "          0.1417],\n",
      "        [ 0.2473, -0.0974,  0.1052,  0.6177,  0.0400,  0.6986,  0.5152,  0.1132,\n",
      "          0.1365],\n",
      "        [ 0.2406, -0.0818,  0.0946,  0.6142,  0.0412,  0.7008,  0.4970,  0.1108,\n",
      "          0.1440],\n",
      "        [ 0.2397, -0.0852,  0.0829,  0.6091,  0.0420,  0.7060,  0.4942,  0.1057,\n",
      "          0.1496],\n",
      "        [ 0.2437, -0.0918,  0.0966,  0.6120,  0.0378,  0.7012,  0.4971,  0.1070,\n",
      "          0.1434],\n",
      "        [ 0.2431, -0.0850,  0.0730,  0.5961,  0.0362,  0.7016,  0.4788,  0.1043,\n",
      "          0.1484],\n",
      "        [ 0.2462, -0.0864,  0.0806,  0.6034,  0.0366,  0.7041,  0.4832,  0.1055,\n",
      "          0.1435],\n",
      "        [ 0.2369, -0.0843,  0.1015,  0.6178,  0.0392,  0.6975,  0.5078,  0.1143,\n",
      "          0.1381],\n",
      "        [ 0.2445, -0.0825,  0.0948,  0.6165,  0.0394,  0.7023,  0.4975,  0.1110,\n",
      "          0.1406],\n",
      "        [ 0.2560, -0.0808,  0.0759,  0.6063,  0.0416,  0.7086,  0.4911,  0.1102,\n",
      "          0.1389],\n",
      "        [ 0.2416, -0.0997,  0.0872,  0.5907,  0.0372,  0.6944,  0.4917,  0.1084,\n",
      "          0.1456],\n",
      "        [ 0.2385, -0.0918,  0.0858,  0.6005,  0.0389,  0.6955,  0.5031,  0.1066,\n",
      "          0.1461],\n",
      "        [ 0.2476, -0.0924,  0.0804,  0.6042,  0.0381,  0.7057,  0.4880,  0.1076,\n",
      "          0.1493],\n",
      "        [ 0.2498, -0.0806,  0.0811,  0.6104,  0.0422,  0.7117,  0.4983,  0.1080,\n",
      "          0.1405],\n",
      "        [ 0.2415, -0.0835,  0.0946,  0.6192,  0.0419,  0.7045,  0.5081,  0.1107,\n",
      "          0.1409],\n",
      "        [ 0.2323, -0.0894,  0.1024,  0.6175,  0.0393,  0.6961,  0.5048,  0.1132,\n",
      "          0.1441],\n",
      "        [ 0.2517, -0.0854,  0.0847,  0.6120,  0.0419,  0.7093,  0.4946,  0.1090,\n",
      "          0.1458],\n",
      "        [ 0.2488, -0.0801,  0.0842,  0.6120,  0.0425,  0.7086,  0.4971,  0.1083,\n",
      "          0.1414],\n",
      "        [ 0.2465, -0.0857,  0.0859,  0.6079,  0.0416,  0.7058,  0.5065,  0.1097,\n",
      "          0.1379],\n",
      "        [ 0.2479, -0.0864,  0.0914,  0.6147,  0.0381,  0.7051,  0.4965,  0.1091,\n",
      "          0.1407],\n",
      "        [ 0.2414, -0.0895,  0.0848,  0.5993,  0.0412,  0.6965,  0.4955,  0.1057,\n",
      "          0.1471],\n",
      "        [ 0.2470, -0.0838,  0.0795,  0.6090,  0.0412,  0.7095,  0.4959,  0.1070,\n",
      "          0.1436],\n",
      "        [ 0.2402, -0.0802,  0.0830,  0.6075,  0.0403,  0.7032,  0.4879,  0.1050,\n",
      "          0.1463],\n",
      "        [ 0.2496, -0.0793,  0.0766,  0.6116,  0.0437,  0.7151,  0.4968,  0.1080,\n",
      "          0.1448],\n",
      "        [ 0.2540, -0.0824,  0.0814,  0.6090,  0.0373,  0.7115,  0.4944,  0.1110,\n",
      "          0.1372],\n",
      "        [ 0.2467, -0.0903,  0.0940,  0.6110,  0.0379,  0.7033,  0.4956,  0.1097,\n",
      "          0.1423],\n",
      "        [ 0.2496, -0.0864,  0.0960,  0.6164,  0.0397,  0.7033,  0.5036,  0.1112,\n",
      "          0.1362],\n",
      "        [ 0.2499, -0.0862,  0.0822,  0.6067,  0.0404,  0.7072,  0.4858,  0.1051,\n",
      "          0.1478],\n",
      "        [ 0.2351, -0.0849,  0.0923,  0.6130,  0.0429,  0.6991,  0.5011,  0.1052,\n",
      "          0.1447],\n",
      "        [ 0.2412, -0.0834,  0.0919,  0.6174,  0.0436,  0.7041,  0.4988,  0.1083,\n",
      "          0.1477],\n",
      "        [ 0.2502, -0.0879,  0.0743,  0.6051,  0.0400,  0.7084,  0.4846,  0.1052,\n",
      "          0.1498],\n",
      "        [ 0.2534, -0.0815,  0.0709,  0.6051,  0.0421,  0.7137,  0.4869,  0.1070,\n",
      "          0.1463],\n",
      "        [ 0.2418, -0.0841,  0.0964,  0.6166,  0.0386,  0.7019,  0.5050,  0.1114,\n",
      "          0.1375],\n",
      "        [ 0.2490, -0.0843,  0.0816,  0.6086,  0.0402,  0.7089,  0.4913,  0.1050,\n",
      "          0.1456],\n",
      "        [ 0.2398, -0.0871,  0.0958,  0.6120,  0.0391,  0.7018,  0.5030,  0.1131,\n",
      "          0.1423],\n",
      "        [ 0.2428, -0.0796,  0.0937,  0.6183,  0.0419,  0.7063,  0.5055,  0.1115,\n",
      "          0.1394],\n",
      "        [ 0.2499, -0.0874,  0.0749,  0.6002,  0.0370,  0.7066,  0.4830,  0.1048,\n",
      "          0.1462],\n",
      "        [ 0.2536, -0.0820,  0.0826,  0.6074,  0.0415,  0.7079,  0.4914,  0.1077,\n",
      "          0.1418],\n",
      "        [ 0.2479, -0.0887,  0.0828,  0.6080,  0.0393,  0.7057,  0.4962,  0.1134,\n",
      "          0.1430],\n",
      "        [ 0.2466, -0.0901,  0.0894,  0.6060,  0.0379,  0.7016,  0.5040,  0.1146,\n",
      "          0.1376],\n",
      "        [ 0.2396, -0.0857,  0.0886,  0.6085,  0.0397,  0.6999,  0.4886,  0.1056,\n",
      "          0.1480],\n",
      "        [ 0.2448, -0.0847,  0.0875,  0.6141,  0.0418,  0.7119,  0.4997,  0.1089,\n",
      "          0.1446],\n",
      "        [ 0.2498, -0.0830,  0.0857,  0.6143,  0.0426,  0.7113,  0.5029,  0.1104,\n",
      "          0.1404],\n",
      "        [ 0.2496, -0.0839,  0.0760,  0.6036,  0.0403,  0.7075,  0.4818,  0.1042,\n",
      "          0.1488],\n",
      "        [ 0.2394, -0.0882,  0.0910,  0.6145,  0.0410,  0.7018,  0.5083,  0.1094,\n",
      "          0.1438],\n",
      "        [ 0.2464, -0.0786,  0.0842,  0.6090,  0.0439,  0.7045,  0.4904,  0.1066,\n",
      "          0.1461],\n",
      "        [ 0.2444, -0.0815,  0.0959,  0.6113,  0.0398,  0.6969,  0.4906,  0.1098,\n",
      "          0.1409],\n",
      "        [ 0.2365, -0.0869,  0.0930,  0.6058,  0.0380,  0.6946,  0.4935,  0.1085,\n",
      "          0.1436],\n",
      "        [ 0.2487, -0.0849,  0.0763,  0.6047,  0.0399,  0.7072,  0.4826,  0.1035,\n",
      "          0.1491],\n",
      "        [ 0.2469, -0.0801,  0.0793,  0.6094,  0.0412,  0.7096,  0.4893,  0.1056,\n",
      "          0.1451],\n",
      "        [ 0.2507, -0.0849,  0.0714,  0.6051,  0.0432,  0.7154,  0.4898,  0.1063,\n",
      "          0.1505],\n",
      "        [ 0.2427, -0.0825,  0.0802,  0.6059,  0.0419,  0.7075,  0.4903,  0.1051,\n",
      "          0.1496],\n",
      "        [ 0.2521, -0.0874,  0.0845,  0.6127,  0.0397,  0.7082,  0.4983,  0.1118,\n",
      "          0.1401],\n",
      "        [ 0.2518, -0.0816,  0.0774,  0.6029,  0.0383,  0.7054,  0.4813,  0.1106,\n",
      "          0.1421],\n",
      "        [ 0.2579, -0.0832,  0.0804,  0.6125,  0.0386,  0.7146,  0.4959,  0.1124,\n",
      "          0.1402],\n",
      "        [ 0.2448, -0.0878,  0.0949,  0.6078,  0.0386,  0.6950,  0.4946,  0.1072,\n",
      "          0.1390],\n",
      "        [ 0.2392, -0.0854,  0.0857,  0.6055,  0.0403,  0.6975,  0.4918,  0.1054,\n",
      "          0.1462],\n",
      "        [ 0.2463, -0.0859,  0.0935,  0.6152,  0.0410,  0.7023,  0.4978,  0.1078,\n",
      "          0.1406]])\n",
      "tensor(0.6493)\n"
     ]
    }
   ],
   "source": [
    "## Loss Test\n",
    "batch = next(iter(validation_loader))\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds = model(batch['features'])\n",
    "\n",
    "print (preds)\n",
    "print(regression_loss(preds, batch['pattern_params']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/maria_korosteleva/Test-Garments-Reconstruction\" target=\"_blank\">https://app.wandb.ai/maria_korosteleva/Test-Garments-Reconstruction</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/maria_korosteleva/Test-Garments-Reconstruction/runs/2dj70d05\" target=\"_blank\">https://app.wandb.ai/maria_korosteleva/Test-Garments-Reconstruction/runs/2dj70d05</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[<wandb.wandb_torch.TorchGraph at 0x22f0fec1400>]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init Weights&biases run\n",
    "#os.environ['WANDB_MODE'] = 'dryrun'\n",
    "\n",
    "import wandb as wb\n",
    "wb.init(name = \"mesh input\", project = 'Test-Garments-Reconstruction')\n",
    "\n",
    "wb.watch(model, log='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "\n",
    "fit(model, regression_loss, optimizer, training_loader)\n",
    "\n",
    "print (\"Finished training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.02655116282403469\n"
     ]
    }
   ],
   "source": [
    "# loss on validation set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    valid_loss = sum([regression_loss(model(batch['features']), batch['pattern_params']) for batch in validation_loader]) \n",
    "\n",
    "print ('Validation loss: {}'.format(valid_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save predictions for validation to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.2462974786758423, 0.7441388368606567, 0.9016931056976318, 0.6540197134017944, 1.1041333675384521, 1.2501575946807861, 1.2502353191375732, 0.8995957374572754, 0.9998902678489685]\n",
      "Saved V9SUOXEHXVLI\n",
      "[1.2463276386260986, 0.7441512942314148, 0.9017415046691895, 0.65404212474823, 1.1041481494903564, 1.2502005100250244, 1.2502684593200684, 0.8996169567108154, 0.9999186396598816]\n",
      "Saved Y6TLQDYOKMKC\n",
      "[1.2463206052780151, 0.7441489696502686, 0.901718258857727, 0.6540318727493286, 1.1041452884674072, 1.2501857280731201, 1.2502539157867432, 0.8996154069900513, 0.9999144673347473]\n",
      "Saved M6PNUHQOZYEF\n",
      "[1.246446132659912, 0.7442108392715454, 0.9018248319625854, 0.6540975570678711, 1.1042542457580566, 1.2503091096878052, 1.2503726482391357, 0.8996891975402832, 1.0000224113464355]\n",
      "Saved X5WIOMUQOEJD\n",
      "[1.2462490797042847, 0.744117021560669, 0.901656985282898, 0.6539962291717529, 1.1040825843811035, 1.250112533569336, 1.2501859664916992, 0.8995614051818848, 0.9998473525047302]\n",
      "Saved E7YOJODRKYQY\n",
      "[1.2464097738265991, 0.7441959977149963, 0.9018012285232544, 0.6540782451629639, 1.1042237281799316, 1.2502741813659668, 1.2503454685211182, 0.8996686935424805, 0.9999881386756897]\n",
      "Saved E9PPPEKYBKID\n",
      "[1.2462801933288574, 0.7441253662109375, 0.9016793966293335, 0.6540095806121826, 1.1041131019592285, 1.2501447200775146, 1.2502198219299316, 0.8995814323425293, 0.9998827576637268]\n",
      "Saved E0AHLPYSPBKE\n",
      "[1.2464090585708618, 0.7441887259483337, 0.9018077850341797, 0.6540844440460205, 1.104222059249878, 1.2502787113189697, 1.25034499168396, 0.8996715545654297, 0.9999884963035583]\n",
      "Saved D2GEWDENUQHH\n",
      "[1.2461501359939575, 0.7440717816352844, 0.901556134223938, 0.6539368629455566, 1.1039988994598389, 1.2500028610229492, 1.2500876188278198, 0.8994966745376587, 0.9997618198394775]\n",
      "Saved J5FTXFOLKGTC\n",
      "[1.2463576793670654, 0.7441674470901489, 0.9017491340637207, 0.6540508270263672, 1.1041843891143799, 1.2502214908599854, 1.2502954006195068, 0.8996360301971436, 0.9999465942382812]\n",
      "Saved I5SRVHVYJOVB\n",
      "[1.2462644577026367, 0.7441179156303406, 0.901675820350647, 0.6540015935897827, 1.1040936708450317, 1.2501299381256104, 1.2502076625823975, 0.8995711803436279, 0.999862551689148]\n",
      "Saved V9SAAHBKEYSD\n",
      "[1.246187686920166, 0.7440862655639648, 0.9015802145004272, 0.6539548635482788, 1.1040363311767578, 1.2500381469726562, 1.2501219511032104, 0.8995175361633301, 0.9997982382774353]\n",
      "Saved B4MGBVXNBZIK\n",
      "[1.2463868856430054, 0.7441824078559875, 0.901792049407959, 0.6540709733963013, 1.1042004823684692, 1.250258207321167, 1.2503244876861572, 0.8996574878692627, 0.9999670386314392]\n",
      "Saved A2UOBZJIFYZI\n",
      "[1.2463340759277344, 0.7441577315330505, 0.9017441272735596, 0.6540448665618896, 1.1041579246520996, 1.2502050399780273, 1.2502775192260742, 0.8996142148971558, 0.9999251961708069]\n",
      "Saved W5TRLFHTVENB\n",
      "[1.2463710308074951, 0.7441746592521667, 0.9017635583877563, 0.6540597677230835, 1.1041927337646484, 1.2502360343933105, 1.2503068447113037, 0.8996416330337524, 0.9999553561210632]\n",
      "Saved L9GHLWTWILNX\n",
      "[1.2464234828948975, 0.7441969513893127, 0.901816725730896, 0.6540879011154175, 1.1042349338531494, 1.2502923011779785, 1.250359296798706, 0.8996789455413818, 1.0000032186508179]\n",
      "Saved H4LKMJJZLKSV\n",
      "[1.2462432384490967, 0.7440993189811707, 0.9016519784927368, 0.6539912223815918, 1.1040737628936768, 1.2501049041748047, 1.2501825094223022, 0.8995553255081177, 0.9998475909233093]\n",
      "Saved K2QOUZMWDIWN\n",
      "[1.2460339069366455, 0.7440168857574463, 0.9014381170272827, 0.6538664102554321, 1.103906273841858, 1.2498774528503418, 1.2499797344207764, 0.8994117975234985, 0.9996687769889832]\n",
      "Saved H4VQNVEJAJOU\n",
      "[1.2463796138763428, 0.7441787123680115, 0.9017812013626099, 0.6540671586990356, 1.1041977405548096, 1.2502492666244507, 1.2503161430358887, 0.8996545076370239, 0.9999626874923706]\n",
      "Saved A0ORXEDOUCFO\n",
      "[1.2464007139205933, 0.7441884875297546, 0.9017881155014038, 0.654075026512146, 1.104217290878296, 1.2502626180648804, 1.2503330707550049, 0.8996624946594238, 0.9999802112579346]\n",
      "Saved H4IJYNLNCXJB\n",
      "[1.2464420795440674, 0.7442020177841187, 0.9018406867980957, 0.6541036367416382, 1.1042510271072388, 1.2503132820129395, 1.2503783702850342, 0.8996996879577637, 1.0000157356262207]\n",
      "Saved L6ZZJWJWPQTO\n",
      "[1.2464516162872314, 0.7442110776901245, 0.901853084564209, 0.6541053652763367, 1.104256510734558, 1.25032639503479, 1.2503899335861206, 0.8997005224227905, 1.0000262260437012]\n",
      "Saved G4JQDUHZFBCZ\n",
      "[1.2463324069976807, 0.7441550493240356, 0.9017213582992554, 0.6540376543998718, 1.1041572093963623, 1.2501921653747559, 1.2502591609954834, 0.8996150493621826, 0.9999234080314636]\n",
      "Saved B9DNRGUZQBJJ\n",
      "[1.2465293407440186, 0.744250476360321, 0.9019238948822021, 0.6541508436203003, 1.1043236255645752, 1.2504044771194458, 1.250462532043457, 0.8997530937194824, 1.0000898838043213]\n",
      "Saved C0IWNQRKACKY\n",
      "[1.2463444471359253, 0.7441593408584595, 0.9017496109008789, 0.6540466547012329, 1.1041682958602905, 1.2502156496047974, 1.2502864599227905, 0.8996244668960571, 0.9999359250068665]\n",
      "Saved T6JDVGYGPKLY\n",
      "[1.246353268623352, 0.7441666722297668, 0.9017562866210938, 0.6540505290031433, 1.104177474975586, 1.2502211332321167, 1.2502975463867188, 0.8996309041976929, 0.9999403357505798]\n",
      "Saved O4ASEXESAFOC\n",
      "[1.246260643005371, 0.7441161870956421, 0.9016715288162231, 0.6540001630783081, 1.104091763496399, 1.250129222869873, 1.2502062320709229, 0.8995680809020996, 0.9998619556427002]\n",
      "Saved W4PSDPFBBYND\n",
      "[1.246199607849121, 0.7441002130508423, 0.9016022682189941, 0.6539631485939026, 1.104040503501892, 1.2500578165054321, 1.2501351833343506, 0.8995294570922852, 0.9998087882995605]\n",
      "Saved A1KFZGQPBSLA\n",
      "[1.246427059173584, 0.7441935539245605, 0.9018280506134033, 0.6540939807891846, 1.1042381525039673, 1.250301480293274, 1.2503674030303955, 0.8996866941452026, 1.0000066757202148]\n",
      "Saved N9QULLYFQBHR\n",
      "[1.2464513778686523, 0.7442131042480469, 0.9018499851226807, 0.6541067361831665, 1.1042572259902954, 1.2503271102905273, 1.250386357307434, 0.8997021913528442, 1.0000274181365967]\n",
      "Saved B0ZLABMCXVSY\n",
      "[1.2463538646697998, 0.7441614270210266, 0.9017543792724609, 0.6540548801422119, 1.1041781902313232, 1.2502202987670898, 1.250288724899292, 0.8996398448944092, 0.9999365210533142]\n",
      "Saved K8ZSVORQUDTL\n",
      "[1.2464162111282349, 0.7441971898078918, 0.9018120765686035, 0.6540858149528503, 1.1042321920394897, 1.25028657913208, 1.2503538131713867, 0.8996741771697998, 0.9999943375587463]\n",
      "Saved D5BWALVOWDAG\n",
      "[1.2463550567626953, 0.744168758392334, 0.9017560482025146, 0.6540517807006836, 1.1041758060455322, 1.2502214908599854, 1.2502920627593994, 0.8996342420578003, 0.999940037727356]\n",
      "Saved F5QFGHIALRFW\n",
      "[1.2463825941085815, 0.7441856265068054, 0.9017752408981323, 0.6540652513504028, 1.1041998863220215, 1.2502472400665283, 1.2503143548965454, 0.8996506929397583, 0.9999645352363586]\n",
      "Saved C3KSGZMKJDKV\n",
      "[1.2463089227676392, 0.7441515922546387, 0.9017149209976196, 0.6540265083312988, 1.1041334867477417, 1.250176191329956, 1.2502467632293701, 0.8995972871780396, 0.9998988509178162]\n",
      "Saved V5SEWXFPFBFS\n",
      "[1.2465695142745972, 0.744270920753479, 0.901957631111145, 0.6541693210601807, 1.1043598651885986, 1.2504472732543945, 1.2505016326904297, 0.8997784852981567, 1.0001293420791626]\n",
      "Saved J1FTTMAZQTAK\n",
      "[1.2464804649353027, 0.7442284822463989, 0.9018748998641968, 0.6541239023208618, 1.1042859554290771, 1.250354528427124, 1.2504137754440308, 0.8997159004211426, 1.0000499486923218]\n",
      "Saved N4OVAEPKWBFN\n",
      "[1.246260643005371, 0.7441186904907227, 0.9016741514205933, 0.6540004014968872, 1.1040902137756348, 1.2501300573349, 1.2502031326293945, 0.8995697498321533, 0.9998642206192017]\n",
      "Saved C7UQLNBLIXLQ\n",
      "[1.246275544166565, 0.7441178560256958, 0.9016828536987305, 0.6540123820304871, 1.104107141494751, 1.2501401901245117, 1.250216007232666, 0.899579644203186, 0.9998746514320374]\n",
      "Saved A2ODYFNZIRBM\n",
      "[1.2463221549987793, 0.7441585659980774, 0.9017237424850464, 0.6540340185165405, 1.104148507118225, 1.2501871585845947, 1.250258207321167, 0.8996082544326782, 0.9999116659164429]\n",
      "Saved I9OHPEDNKSYS\n",
      "[1.2463579177856445, 0.7441549897193909, 0.9017566442489624, 0.6540542840957642, 1.104180097579956, 1.2502254247665405, 1.2502992153167725, 0.8996376991271973, 0.9999479651451111]\n",
      "Saved H0DAHNXQTIKV\n",
      "[1.2463369369506836, 0.7441554069519043, 0.9017369747161865, 0.6540431976318359, 1.1041572093963623, 1.250196933746338, 1.2502681016921997, 0.899626612663269, 0.9999206066131592]\n",
      "Saved Y7QMGGINLGGC\n",
      "[1.2464046478271484, 0.7441933155059814, 0.9018003940582275, 0.6540780067443848, 1.1042182445526123, 1.250271201133728, 1.2503385543823242, 0.8996680974960327, 0.9999813437461853]\n",
      "Saved G7DLDOOTHQNV\n",
      "[1.2463858127593994, 0.7441849112510681, 0.9017952680587769, 0.6540721654891968, 1.1041995286941528, 1.2502593994140625, 1.2503256797790527, 0.8996551036834717, 0.9999670386314392]\n",
      "Saved U0JPHWOTCKTF\n",
      "[1.246516466140747, 0.7442446947097778, 0.9019030332565308, 0.6541383266448975, 1.1043150424957275, 1.2503902912139893, 1.2504496574401855, 0.8997437953948975, 1.0000848770141602]\n",
      "Saved B6VDHQRVDUZK\n",
      "[1.2464241981506348, 0.7442011833190918, 0.9018139839172363, 0.6540881395339966, 1.1042362451553345, 1.2502893209457397, 1.25035560131073, 0.8996783494949341, 1.0000009536743164]\n",
      "Saved P1YABTHUBXLI\n",
      "[1.246381402015686, 0.7441819906234741, 0.9017833471298218, 0.6540670394897461, 1.1041984558105469, 1.2502493858337402, 1.2503201961517334, 0.8996517658233643, 0.9999629259109497]\n",
      "Saved P8BLXXVFSUEB\n",
      "[1.2464873790740967, 0.7442256212234497, 0.9018880128860474, 0.6541252136230469, 1.1042896509170532, 1.250368356704712, 1.2504299879074097, 0.8997231721878052, 1.0000624656677246]\n",
      "Saved X6IQJRQQRJWS\n",
      "[1.246260643005371, 0.7441246509552002, 0.9016567468643188, 0.653998851776123, 1.1040966510772705, 1.2501170635223389, 1.250194787979126, 0.8995673656463623, 0.99985671043396]\n",
      "Saved N2LQTZGOVDDP\n",
      "[1.246361494064331, 0.7441731691360474, 0.9017444849014282, 0.6540499925613403, 1.104177713394165, 1.2502179145812988, 1.2502856254577637, 0.8996337652206421, 0.999944806098938]\n",
      "Saved Z7DPNMVQSYIY\n",
      "[1.246422290802002, 0.7441942095756531, 0.9018405675888062, 0.6540892720222473, 1.1042317152023315, 1.2503042221069336, 1.250377893447876, 0.8996762037277222, 1.000007152557373]\n",
      "Saved N9DPWNNQMFQS\n",
      "[1.2463679313659668, 0.7441725134849548, 0.9017788171768188, 0.6540637016296387, 1.1041866540908813, 1.2502405643463135, 1.2503104209899902, 0.8996390104293823, 0.9999527931213379]\n",
      "Saved N3MOMNYBSSYD\n",
      "[1.2464196681976318, 0.7441986799240112, 0.9018102884292603, 0.6540880799293518, 1.1042330265045166, 1.2502851486206055, 1.250352382659912, 0.8996745347976685, 0.9999943971633911]\n",
      "Saved O2OGNOYRKTWS\n",
      "[1.2465155124664307, 0.7442449927330017, 0.901907205581665, 0.6541401147842407, 1.1043156385421753, 1.2503927946090698, 1.250450849533081, 0.8997421264648438, 1.000083327293396]\n",
      "Saved S1PHEJYCZMYB\n",
      "[1.2465040683746338, 0.7442333698272705, 0.9019128084182739, 0.6541405916213989, 1.104301929473877, 1.2503857612609863, 1.2504465579986572, 0.8997364044189453, 1.000070571899414]\n",
      "Saved M3JDZYFXHRMZ\n",
      "[1.2463295459747314, 0.7441583871841431, 0.9017249345779419, 0.654036283493042, 1.104154348373413, 1.2501895427703857, 1.2502624988555908, 0.8996162414550781, 0.9999157190322876]\n",
      "Saved S0DHSXOWJOEY\n",
      "[1.2463080883026123, 0.7441398501396179, 0.9017176628112793, 0.6540290117263794, 1.104134440422058, 1.2501771450042725, 1.2502509355545044, 0.8996015787124634, 0.9999043941497803]\n",
      "Saved L1EJTOZTZKKQ\n",
      "[1.2462100982666016, 0.7440946102142334, 0.9016152620315552, 0.6539736986160278, 1.1040525436401367, 1.2500677108764648, 1.250147819519043, 0.8995331525802612, 0.9998158812522888]\n",
      "Saved C6LPFAORSVOF\n",
      "[1.2462937831878662, 0.744132936000824, 0.9016826152801514, 0.6540138721466064, 1.1041243076324463, 1.2501509189605713, 1.2502284049987793, 0.8995895385742188, 0.9998916983604431]\n",
      "Saved D4XVRSPGQHKC\n",
      "[1.2462396621704102, 0.74411541223526, 0.9016293287277222, 0.6539794206619263, 1.104081392288208, 1.250091552734375, 1.2501753568649292, 0.8995494842529297, 0.9998432993888855]\n",
      "Saved L1WQEFFTHSPQ\n",
      "[1.246402382850647, 0.7441871166229248, 0.9018046855926514, 0.654080867767334, 1.104215145111084, 1.2502686977386475, 1.2503392696380615, 0.8996678590774536, 0.9999754428863525]\n",
      "Saved W0QDPWNZKUHD\n",
      "[1.2464404106140137, 0.7442125082015991, 0.9018380641937256, 0.6541042327880859, 1.1042630672454834, 1.2503271102905273, 1.250385046005249, 0.8997001647949219, 1.0000312328338623]\n",
      "Saved M8HCNOCCKLXA\n",
      "[1.2463860511779785, 0.7441823482513428, 0.9017891883850098, 0.6540703773498535, 1.1042038202285767, 1.2502577304840088, 1.2503247261047363, 0.8996541500091553, 0.9999699592590332]\n",
      "Saved A3BYKOUWHLMC\n",
      "[1.2464386224746704, 0.744208037853241, 0.9018220901489258, 0.6540950536727905, 1.1042503118515015, 1.2503042221069336, 1.2503689527511597, 0.8996886014938354, 1.0000154972076416]\n",
      "Saved D1BZXWAWXNHF\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    batch = next(iter(validation_loader))    # might have some issues, see https://github.com/pytorch/pytorch/issues/1917\n",
    "    shirt_dataset_normalized.save_prediction_batch(model(batch['features']), batch['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
