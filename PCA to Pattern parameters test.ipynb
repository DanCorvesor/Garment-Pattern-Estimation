{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducibility when testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x21d984278d0>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see https://pytorch.org/docs/stable/notes/randomness.html\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# when using cuda\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import scipy.io  # for reading matlab matrix\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = 'D:\\Data\\CLOTHING\\Learning Shared Shape Space_shirt_dataset_rest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom DataSet class\n",
    "class ParametrizedShirtDataSet(Dataset):\n",
    "    \"\"\"\n",
    "    For loading the data of \"Learning Shared Shape Space..\" paper\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir, transform = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the t-shirt examples as subfolders\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "        self.root_path = Path(root_dir)\n",
    "        \n",
    "        # list of items = subfolders\n",
    "        self.datapoints_names = next(os.walk(self.root_path))[1]\n",
    "        \n",
    "        # datapoint folder structure\n",
    "        self.mesh_filename = 'shirt_mesh_r.obj'\n",
    "        self.pattern_params_filename = 'shirt_info.txt'\n",
    "        self.features_filename = 'visfea.mat'\n",
    "        \n",
    "    def update_transform(self, transform):\n",
    "        \"\"\"apply new transform when loading the data\"\"\"\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Number of entries in the dataset\"\"\"\n",
    "        return len(self.datapoints_names)\n",
    "    \n",
    "    \n",
    "    def read_features(self, datapoint_name):\n",
    "        \"\"\"features parameters from a given datapoint subfolder\"\"\"\n",
    "        assert (self.root_path / datapoint_name / self.features_filename).exists()\n",
    "        \n",
    "        matlab_mat = scipy.io.loadmat(self.root_path / datapoint_name / self.features_filename)\n",
    "        # assuming fea1 is what we need\n",
    "        return np.asarray(matlab_mat['fea2']).squeeze()\n",
    "    \n",
    "    \n",
    "    def read_pattern_params(self, datapoint_name):\n",
    "        \"\"\"9 pattern size parameters from a given datapoint subfolder\"\"\"\n",
    "        assert (self.root_path / datapoint_name / self.pattern_params_filename).exists()\n",
    "        \n",
    "        # assuming that we need the numbers from the last line in file\n",
    "        with open(self.root_path / datapoint_name / self.pattern_params_filename) as f:\n",
    "            lines = f.readlines()\n",
    "            params = np.fromstring(lines[-1],  sep = ' ')\n",
    "        return params\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Called when indexing: read the corresponding data. \n",
    "        Does not support list indexing\"\"\"\n",
    "        \n",
    "        if torch.is_tensor(idx):  # allow indexing by tensors\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        datapoint_name = self.datapoints_names[idx]\n",
    "        \n",
    "        features = self.read_features(datapoint_name)\n",
    "        \n",
    "        # read the pattern parameters\n",
    "        pattern_parameters = self.read_pattern_params(datapoint_name)\n",
    "        \n",
    "        sample = {'features': features, 'pattern_params': pattern_parameters, 'name': datapoint_name}\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample\n",
    "        \n",
    "        \n",
    "    def save_prediction_batch(self, predicted_params, names):\n",
    "        \"\"\"Saves predicted params of the datapoint to the original data folder\"\"\"\n",
    "        \n",
    "        for prediction, name in zip(predicted_params, names):\n",
    "            path_to_prediction = self.root_path / '..' / 'predictions' / name\n",
    "            try:\n",
    "                os.makedirs(path_to_prediction)\n",
    "            except OSError:\n",
    "                pass\n",
    "            \n",
    "            prediction = prediction.tolist()\n",
    "            with open(path_to_prediction / self.pattern_params_filename, 'w+') as f:\n",
    "                f.writelines(['0\\n', '0\\n', ' '.join(map(str, prediction))])\n",
    "                print ('Saved ' + name)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transforms -- to tensor\n",
    "class SampleToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        features, params = sample['features'], sample['pattern_params']\n",
    "        \n",
    "        return {\n",
    "                'features': torch.from_numpy(features).float(), \n",
    "                'pattern_params': torch.from_numpy(params).float(), \n",
    "                'name': sample['name']\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transforms -- normalize\n",
    "class NormalizeInputfeatures(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "    def __init__(self, mean_features, std_features):\n",
    "        self.mean = mean_features\n",
    "        self.std = std_features\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        features = sample['features']\n",
    "        \n",
    "        return {\n",
    "                    'features': torch.div((features - self.mean), self.std), \n",
    "                    'pattern_params': sample['pattern_params'], \n",
    "                    'name': sample['name']\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Normalization?\n",
    "\n",
    "def get_mean_std(dataloader):\n",
    "    \n",
    "    stats = { 'batch_sums' : [], 'batch_sq_sums' : []}\n",
    "    \n",
    "    for data in dataloader:\n",
    "        batch_sum = data['features'].sum(0)\n",
    "        stats['batch_sums'].append(batch_sum)\n",
    "\n",
    "    mean_features = sum(stats['batch_sums']) / len(dataloader)\n",
    "    \n",
    "    for data in dataloader:\n",
    "        batch_sum_sq = (data['features'] - mean_features.view(1, len(mean_features)))**2\n",
    "        stats['batch_sq_sums'].append(batch_sum_sq.sum(0))\n",
    "                        \n",
    "    std_features = torch.sqrt(sum(stats['batch_sq_sums']) / len(dataloader))\n",
    "    \n",
    "    return mean_features, std_features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1050\n",
      "torch.Size([100])\n",
      "torch.Size([9])\n"
     ]
    }
   ],
   "source": [
    "# test loading\n",
    "dataset = ParametrizedShirtDataSet(Path(data_location), \n",
    "                                  SampleToTensor())\n",
    "\n",
    "print (len(dataset))\n",
    "print (dataset[1000]['features'].shape)\n",
    "print (dataset[1000]['pattern_params'].shape)\n",
    "#print (dataset[1000])\n",
    "\n",
    "loader = DataLoader(dataset, 10, shuffle=True)\n",
    "\n",
    "mean, std = get_mean_std(loader)\n",
    "#print (mean, std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4847, 1.7808, 2.5189, 2.8437, 4.7678, 6.1286, 6.0374, 6.0461, 6.0504,\n",
      "        6.0500, 6.0500, 6.0500, 6.0501, 6.0495, 6.0338, 6.0290, 6.0191, 6.0103,\n",
      "        6.0033, 5.9944, 5.9856, 5.9771, 5.9658, 5.9569, 5.9481, 5.9396, 5.9291,\n",
      "        5.9197, 5.9068, 5.9005, 5.7688, 5.1725, 4.5954, 3.9547, 2.9973, 2.3022,\n",
      "        2.2025, 2.1716, 2.1283, 2.0894, 2.0596, 2.0363, 2.0191, 2.0048, 1.9907,\n",
      "        1.9804, 1.9743, 1.9572, 1.9458, 1.9341, 1.9225, 1.9199, 1.9182, 1.9112,\n",
      "        1.9119, 1.9027, 1.9047, 1.8997, 1.9000, 1.9002, 1.8983, 1.8905, 1.8855,\n",
      "        1.8806, 1.8821, 1.8798, 1.8794, 1.8862, 1.9059, 1.9212, 1.9365, 1.9526,\n",
      "        1.9691, 1.9886, 2.0051, 2.0236, 2.0428, 2.0614, 2.0781, 2.0829, 2.1120,\n",
      "        2.1466, 2.1775, 2.2130, 2.2419, 2.2715, 2.3000, 2.3252, 2.3484, 2.3717,\n",
      "        2.3947, 2.4142, 2.4290, 2.4452, 2.4613, 2.4773, 2.5043, 2.3297, 1.7479,\n",
      "        0.7207])\n",
      "tensor([-0.3243, -0.3104, -0.3113, -0.3173, -0.3096, -0.3099, -0.3147, -0.3157,\n",
      "        -0.3159, -0.3160, -0.3160, -0.3160, -0.3159, -0.3159, -0.3160, -0.3159,\n",
      "        -0.3159, -0.3159, -0.3158, -0.3157, -0.3155, -0.3153, -0.3149, -0.3144,\n",
      "        -0.3138, -0.3131, -0.3122, -0.3112, -0.3100, -0.3084, -0.3075, -0.3099,\n",
      "        -0.3123, -0.3155, -0.3221, -0.3272, -0.3264, -0.3248, -0.3236, -0.3226,\n",
      "        -0.3215, -0.3206, -0.3198, -0.3193, -0.3190, -0.3188, -0.3186, -0.3186,\n",
      "        -0.3186, -0.3187, -0.3188, -0.3188, -0.3187, -0.3188, -0.3187, -0.3188,\n",
      "        -0.3187, -0.3187, -0.3187, -0.3187, -0.3187, -0.3188, -0.3189, -0.3190,\n",
      "        -0.3190, -0.3190, -0.3191, -0.3190, -0.3188, -0.3187, -0.3186, -0.3185,\n",
      "        -0.3184, -0.3183, -0.3182, -0.3181, -0.3180, -0.3178, -0.3177, -0.3179,\n",
      "        -0.3176, -0.3172, -0.3169, -0.3165, -0.3162, -0.3159, -0.3158, -0.3157,\n",
      "        -0.3155, -0.3153, -0.3151, -0.3150, -0.3150, -0.3149, -0.3148, -0.3148,\n",
      "        -0.3143, -0.3154, -0.3190, -0.3238])\n"
     ]
    }
   ],
   "source": [
    "dataset_normalized = ParametrizedShirtDataSet(Path(data_location), \n",
    "                                  transforms.Compose([\n",
    "                                      SampleToTensor(), \n",
    "                                      NormalizeInputfeatures(mean, std)]))\n",
    "\n",
    "print (dataset[1]['features'])\n",
    "print (dataset_normalized[1]['features'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShirtfeaturesMLP(nn.Module):\n",
    "    \"\"\"MLP for training on shirts dataset. Assumes 100 features parameters used\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # layers definitions\n",
    "        self.sequence = nn.Sequential(\n",
    "            nn.Linear(100, 120), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(120, 80), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(80, 60),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(60, 9)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x_batch):\n",
    "        return self.sequence(x_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShirtfeaturesMLP(\n",
      "  (sequence): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=120, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=120, out_features=80, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=80, out_features=60, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=60, out_features=9, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = ShirtfeaturesMLP()\n",
    "\n",
    "print (net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Parameters\n",
    "batch_size = 64\n",
    "epochs_num = 100\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data normalization\n",
    "shirt_dataset = ParametrizedShirtDataSet(Path(data_location), \n",
    "                                  SampleToTensor())\n",
    "\n",
    "mean, std = get_mean_std(DataLoader(shirt_dataset, 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split: 945 / 105\n"
     ]
    }
   ],
   "source": [
    "# Data load and split\n",
    "shirt_dataset_normalized = ParametrizedShirtDataSet(Path(data_location), \n",
    "                                  transforms.Compose([\n",
    "                                      SampleToTensor(), \n",
    "                                      NormalizeInputfeatures(mean, std)]))\n",
    "\n",
    "valid_size = (int) (len(shirt_dataset_normalized) / 10)\n",
    "# split is RANDOM. Might affect performance\n",
    "training_set, validation_set = torch.utils.data.random_split(\n",
    "    shirt_dataset_normalized, \n",
    "    (len(shirt_dataset) - valid_size, valid_size))\n",
    "\n",
    "print ('Split: {} / {}'.format(len(training_set), len(validation_set)))\n",
    "\n",
    "training_loader = DataLoader(training_set, batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_set, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop func\n",
    "\n",
    "def fit(model, regression_loss, optimizer, train_loader):\n",
    "    \n",
    "    for epoch in range (epochs_num):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, batch in enumerate(training_loader):\n",
    "            preds = model(batch['features'])\n",
    "            \n",
    "            loss = regression_loss(preds, batch['pattern_params'])\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # logging\n",
    "            if i % 5 == 4:\n",
    "                wb.log({'epoch': epoch, 'loss': loss})\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(\n",
    "                *[(regression_loss(model(batch['features']), batch['pattern_params']), len(batch)) for batch in validation_loader]\n",
    "            )\n",
    "            \n",
    "        valid_loss = np.sum(losses) / np.sum(nums)\n",
    "        print ('Epoch: {}, Validation Loss: {}'.format(epoch, valid_loss))\n",
    "        wb.log({'epoch': epoch, 'valid_loss': valid_loss})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the definitions\n",
    "# model\n",
    "model = ShirtfeaturesMLP()\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "# loss function\n",
    "regression_loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            Using <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> in dryrun mode. Not logging results to the cloud.<br/>\n",
       "            Call wandb.login() to authenticate this machine.<br/>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[<wandb.wandb_torch.TorchGraph at 0x21d95c40a90>]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init Weights&biases run\n",
    "os.environ['WANDB_MODE'] = 'dryrun'\n",
    "\n",
    "import wandb as wb\n",
    "wb.init(name = \"fea2 + data normalization + logging tests2\", project = 'Test-Garments-Reconstruction')\n",
    "\n",
    "wb.watch(model, log='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Validation Loss: 0.12410638729731242\n",
      "Epoch: 1, Validation Loss: 0.013951104134321213\n",
      "Epoch: 2, Validation Loss: 0.004635093423227469\n",
      "Epoch: 3, Validation Loss: 0.004422236544390519\n",
      "Epoch: 4, Validation Loss: 0.004410682246088982\n",
      "Epoch: 5, Validation Loss: 0.004420716315507889\n",
      "Epoch: 6, Validation Loss: 0.004422058661778768\n",
      "Epoch: 7, Validation Loss: 0.0044191448638836546\n",
      "Epoch: 8, Validation Loss: 0.004418723285198212\n",
      "Epoch: 9, Validation Loss: 0.004424821895857652\n",
      "Epoch: 10, Validation Loss: 0.004421636772652467\n",
      "Epoch: 11, Validation Loss: 0.004412914626300335\n",
      "Epoch: 12, Validation Loss: 0.004411815976103147\n",
      "Epoch: 13, Validation Loss: 0.004410261909166972\n",
      "Epoch: 14, Validation Loss: 0.004411476664245129\n",
      "Epoch: 15, Validation Loss: 0.004414470245440801\n",
      "Epoch: 16, Validation Loss: 0.004416363313794136\n",
      "Epoch: 17, Validation Loss: 0.004414473970731099\n",
      "Epoch: 18, Validation Loss: 0.004412018383542697\n",
      "Epoch: 19, Validation Loss: 0.004412309577067693\n",
      "Epoch: 20, Validation Loss: 0.004420057249565919\n",
      "Epoch: 21, Validation Loss: 0.004418447924156983\n",
      "Epoch: 22, Validation Loss: 0.004418153315782547\n",
      "Epoch: 23, Validation Loss: 0.004415431059896946\n",
      "Epoch: 24, Validation Loss: 0.004415617634852727\n",
      "Epoch: 25, Validation Loss: 0.004421863084038098\n",
      "Epoch: 26, Validation Loss: 0.0044301121185223264\n",
      "Epoch: 27, Validation Loss: 0.004416257453461488\n",
      "Epoch: 28, Validation Loss: 0.0044253865877787275\n",
      "Epoch: 29, Validation Loss: 0.004430092250307401\n",
      "Epoch: 30, Validation Loss: 0.004417693863312404\n",
      "Epoch: 31, Validation Loss: 0.004418812692165375\n",
      "Epoch: 32, Validation Loss: 0.004419334853688876\n",
      "Epoch: 33, Validation Loss: 0.004419818830986817\n",
      "Epoch: 34, Validation Loss: 0.004416688034931819\n",
      "Epoch: 35, Validation Loss: 0.004421345268686612\n",
      "Epoch: 36, Validation Loss: 0.0044185879329840345\n",
      "Epoch: 37, Validation Loss: 0.0044205936913688975\n",
      "Epoch: 38, Validation Loss: 0.004413985957702001\n",
      "Epoch: 39, Validation Loss: 0.004414041216174762\n",
      "Epoch: 40, Validation Loss: 0.0044139716774225235\n",
      "Epoch: 41, Validation Loss: 0.004417211438218753\n",
      "Epoch: 42, Validation Loss: 0.004419344787796338\n",
      "Epoch: 43, Validation Loss: 0.004417672132452329\n",
      "Epoch: 44, Validation Loss: 0.004424134890238444\n",
      "Epoch: 45, Validation Loss: 0.004430223256349564\n",
      "Epoch: 46, Validation Loss: 0.00442119004825751\n",
      "Epoch: 47, Validation Loss: 0.004421216435730457\n",
      "Epoch: 48, Validation Loss: 0.004420393457015355\n",
      "Epoch: 49, Validation Loss: 0.0044176724428931875\n",
      "Epoch: 50, Validation Loss: 0.004417818039655685\n",
      "Epoch: 51, Validation Loss: 0.004414750263094902\n",
      "Epoch: 52, Validation Loss: 0.004411319270730019\n",
      "Epoch: 53, Validation Loss: 0.0044182172665993375\n",
      "Epoch: 54, Validation Loss: 0.004423689718047778\n",
      "Epoch: 55, Validation Loss: 0.0044144199540217715\n",
      "Epoch: 56, Validation Loss: 0.004424626628557841\n",
      "Epoch: 57, Validation Loss: 0.004417105267445247\n",
      "Epoch: 58, Validation Loss: 0.004430324460069339\n",
      "Epoch: 59, Validation Loss: 0.00441952608525753\n",
      "Epoch: 60, Validation Loss: 0.004420797651012738\n",
      "Epoch: 61, Validation Loss: 0.004418106749653816\n",
      "Epoch: 62, Validation Loss: 0.004418869192401568\n",
      "Epoch: 63, Validation Loss: 0.004431352640191714\n",
      "Epoch: 64, Validation Loss: 0.004421557920674483\n",
      "Epoch: 65, Validation Loss: 0.00441789689163367\n",
      "Epoch: 66, Validation Loss: 0.004425126438339551\n",
      "Epoch: 67, Validation Loss: 0.004435655350486438\n",
      "Epoch: 68, Validation Loss: 0.004420824969808261\n",
      "Epoch: 69, Validation Loss: 0.004419548436999321\n",
      "Epoch: 70, Validation Loss: 0.00442283662656943\n",
      "Epoch: 71, Validation Loss: 0.004431219771504402\n",
      "Epoch: 72, Validation Loss: 0.0044201140602429705\n",
      "Epoch: 73, Validation Loss: 0.004421532774964969\n",
      "Epoch: 74, Validation Loss: 0.004425576577583949\n",
      "Epoch: 75, Validation Loss: 0.0044221015026172\n",
      "Epoch: 76, Validation Loss: 0.004416331648826599\n",
      "Epoch: 77, Validation Loss: 0.004429848864674568\n",
      "Epoch: 78, Validation Loss: 0.004423487310608228\n",
      "Epoch: 79, Validation Loss: 0.00441566730539004\n",
      "Epoch: 80, Validation Loss: 0.004413709665338199\n",
      "Epoch: 81, Validation Loss: 0.004424725038309892\n",
      "Epoch: 82, Validation Loss: 0.004412270771960418\n",
      "Epoch: 83, Validation Loss: 0.004414255420366923\n",
      "Epoch: 84, Validation Loss: 0.004423523632188638\n",
      "Epoch: 85, Validation Loss: 0.004419700553019841\n",
      "Epoch: 86, Validation Loss: 0.004421011234323184\n",
      "Epoch: 87, Validation Loss: 0.004414799933632215\n",
      "Epoch: 88, Validation Loss: 0.0044160038232803345\n",
      "Epoch: 89, Validation Loss: 0.004423130303621292\n",
      "Epoch: 90, Validation Loss: 0.004416886096199353\n",
      "Epoch: 91, Validation Loss: 0.004417491145431995\n",
      "Epoch: 92, Validation Loss: 0.004425373549262683\n",
      "Epoch: 93, Validation Loss: 0.0044152624905109406\n",
      "Epoch: 94, Validation Loss: 0.004416411742568016\n",
      "Epoch: 95, Validation Loss: 0.00442041518787543\n",
      "Epoch: 96, Validation Loss: 0.004414940252900124\n",
      "Epoch: 97, Validation Loss: 0.004416918692489465\n",
      "Epoch: 98, Validation Loss: 0.00442201333741347\n",
      "Epoch: 99, Validation Loss: 0.004425193804005782\n",
      "Finished training\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "\n",
    "fit(model, regression_loss, optimizer, training_loader)\n",
    "\n",
    "print (\"Finished training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.02655116282403469\n"
     ]
    }
   ],
   "source": [
    "# loss on validation set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    valid_loss = sum([regression_loss(model(batch['features']), batch['pattern_params']) for batch in validation_loader]) \n",
    "\n",
    "print ('Validation loss: {}'.format(valid_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save predictions for validation to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.2462974786758423, 0.7441388368606567, 0.9016931056976318, 0.6540197134017944, 1.1041333675384521, 1.2501575946807861, 1.2502353191375732, 0.8995957374572754, 0.9998902678489685]\n",
      "Saved V9SUOXEHXVLI\n",
      "[1.2463276386260986, 0.7441512942314148, 0.9017415046691895, 0.65404212474823, 1.1041481494903564, 1.2502005100250244, 1.2502684593200684, 0.8996169567108154, 0.9999186396598816]\n",
      "Saved Y6TLQDYOKMKC\n",
      "[1.2463206052780151, 0.7441489696502686, 0.901718258857727, 0.6540318727493286, 1.1041452884674072, 1.2501857280731201, 1.2502539157867432, 0.8996154069900513, 0.9999144673347473]\n",
      "Saved M6PNUHQOZYEF\n",
      "[1.246446132659912, 0.7442108392715454, 0.9018248319625854, 0.6540975570678711, 1.1042542457580566, 1.2503091096878052, 1.2503726482391357, 0.8996891975402832, 1.0000224113464355]\n",
      "Saved X5WIOMUQOEJD\n",
      "[1.2462490797042847, 0.744117021560669, 0.901656985282898, 0.6539962291717529, 1.1040825843811035, 1.250112533569336, 1.2501859664916992, 0.8995614051818848, 0.9998473525047302]\n",
      "Saved E7YOJODRKYQY\n",
      "[1.2464097738265991, 0.7441959977149963, 0.9018012285232544, 0.6540782451629639, 1.1042237281799316, 1.2502741813659668, 1.2503454685211182, 0.8996686935424805, 0.9999881386756897]\n",
      "Saved E9PPPEKYBKID\n",
      "[1.2462801933288574, 0.7441253662109375, 0.9016793966293335, 0.6540095806121826, 1.1041131019592285, 1.2501447200775146, 1.2502198219299316, 0.8995814323425293, 0.9998827576637268]\n",
      "Saved E0AHLPYSPBKE\n",
      "[1.2464090585708618, 0.7441887259483337, 0.9018077850341797, 0.6540844440460205, 1.104222059249878, 1.2502787113189697, 1.25034499168396, 0.8996715545654297, 0.9999884963035583]\n",
      "Saved D2GEWDENUQHH\n",
      "[1.2461501359939575, 0.7440717816352844, 0.901556134223938, 0.6539368629455566, 1.1039988994598389, 1.2500028610229492, 1.2500876188278198, 0.8994966745376587, 0.9997618198394775]\n",
      "Saved J5FTXFOLKGTC\n",
      "[1.2463576793670654, 0.7441674470901489, 0.9017491340637207, 0.6540508270263672, 1.1041843891143799, 1.2502214908599854, 1.2502954006195068, 0.8996360301971436, 0.9999465942382812]\n",
      "Saved I5SRVHVYJOVB\n",
      "[1.2462644577026367, 0.7441179156303406, 0.901675820350647, 0.6540015935897827, 1.1040936708450317, 1.2501299381256104, 1.2502076625823975, 0.8995711803436279, 0.999862551689148]\n",
      "Saved V9SAAHBKEYSD\n",
      "[1.246187686920166, 0.7440862655639648, 0.9015802145004272, 0.6539548635482788, 1.1040363311767578, 1.2500381469726562, 1.2501219511032104, 0.8995175361633301, 0.9997982382774353]\n",
      "Saved B4MGBVXNBZIK\n",
      "[1.2463868856430054, 0.7441824078559875, 0.901792049407959, 0.6540709733963013, 1.1042004823684692, 1.250258207321167, 1.2503244876861572, 0.8996574878692627, 0.9999670386314392]\n",
      "Saved A2UOBZJIFYZI\n",
      "[1.2463340759277344, 0.7441577315330505, 0.9017441272735596, 0.6540448665618896, 1.1041579246520996, 1.2502050399780273, 1.2502775192260742, 0.8996142148971558, 0.9999251961708069]\n",
      "Saved W5TRLFHTVENB\n",
      "[1.2463710308074951, 0.7441746592521667, 0.9017635583877563, 0.6540597677230835, 1.1041927337646484, 1.2502360343933105, 1.2503068447113037, 0.8996416330337524, 0.9999553561210632]\n",
      "Saved L9GHLWTWILNX\n",
      "[1.2464234828948975, 0.7441969513893127, 0.901816725730896, 0.6540879011154175, 1.1042349338531494, 1.2502923011779785, 1.250359296798706, 0.8996789455413818, 1.0000032186508179]\n",
      "Saved H4LKMJJZLKSV\n",
      "[1.2462432384490967, 0.7440993189811707, 0.9016519784927368, 0.6539912223815918, 1.1040737628936768, 1.2501049041748047, 1.2501825094223022, 0.8995553255081177, 0.9998475909233093]\n",
      "Saved K2QOUZMWDIWN\n",
      "[1.2460339069366455, 0.7440168857574463, 0.9014381170272827, 0.6538664102554321, 1.103906273841858, 1.2498774528503418, 1.2499797344207764, 0.8994117975234985, 0.9996687769889832]\n",
      "Saved H4VQNVEJAJOU\n",
      "[1.2463796138763428, 0.7441787123680115, 0.9017812013626099, 0.6540671586990356, 1.1041977405548096, 1.2502492666244507, 1.2503161430358887, 0.8996545076370239, 0.9999626874923706]\n",
      "Saved A0ORXEDOUCFO\n",
      "[1.2464007139205933, 0.7441884875297546, 0.9017881155014038, 0.654075026512146, 1.104217290878296, 1.2502626180648804, 1.2503330707550049, 0.8996624946594238, 0.9999802112579346]\n",
      "Saved H4IJYNLNCXJB\n",
      "[1.2464420795440674, 0.7442020177841187, 0.9018406867980957, 0.6541036367416382, 1.1042510271072388, 1.2503132820129395, 1.2503783702850342, 0.8996996879577637, 1.0000157356262207]\n",
      "Saved L6ZZJWJWPQTO\n",
      "[1.2464516162872314, 0.7442110776901245, 0.901853084564209, 0.6541053652763367, 1.104256510734558, 1.25032639503479, 1.2503899335861206, 0.8997005224227905, 1.0000262260437012]\n",
      "Saved G4JQDUHZFBCZ\n",
      "[1.2463324069976807, 0.7441550493240356, 0.9017213582992554, 0.6540376543998718, 1.1041572093963623, 1.2501921653747559, 1.2502591609954834, 0.8996150493621826, 0.9999234080314636]\n",
      "Saved B9DNRGUZQBJJ\n",
      "[1.2465293407440186, 0.744250476360321, 0.9019238948822021, 0.6541508436203003, 1.1043236255645752, 1.2504044771194458, 1.250462532043457, 0.8997530937194824, 1.0000898838043213]\n",
      "Saved C0IWNQRKACKY\n",
      "[1.2463444471359253, 0.7441593408584595, 0.9017496109008789, 0.6540466547012329, 1.1041682958602905, 1.2502156496047974, 1.2502864599227905, 0.8996244668960571, 0.9999359250068665]\n",
      "Saved T6JDVGYGPKLY\n",
      "[1.246353268623352, 0.7441666722297668, 0.9017562866210938, 0.6540505290031433, 1.104177474975586, 1.2502211332321167, 1.2502975463867188, 0.8996309041976929, 0.9999403357505798]\n",
      "Saved O4ASEXESAFOC\n",
      "[1.246260643005371, 0.7441161870956421, 0.9016715288162231, 0.6540001630783081, 1.104091763496399, 1.250129222869873, 1.2502062320709229, 0.8995680809020996, 0.9998619556427002]\n",
      "Saved W4PSDPFBBYND\n",
      "[1.246199607849121, 0.7441002130508423, 0.9016022682189941, 0.6539631485939026, 1.104040503501892, 1.2500578165054321, 1.2501351833343506, 0.8995294570922852, 0.9998087882995605]\n",
      "Saved A1KFZGQPBSLA\n",
      "[1.246427059173584, 0.7441935539245605, 0.9018280506134033, 0.6540939807891846, 1.1042381525039673, 1.250301480293274, 1.2503674030303955, 0.8996866941452026, 1.0000066757202148]\n",
      "Saved N9QULLYFQBHR\n",
      "[1.2464513778686523, 0.7442131042480469, 0.9018499851226807, 0.6541067361831665, 1.1042572259902954, 1.2503271102905273, 1.250386357307434, 0.8997021913528442, 1.0000274181365967]\n",
      "Saved B0ZLABMCXVSY\n",
      "[1.2463538646697998, 0.7441614270210266, 0.9017543792724609, 0.6540548801422119, 1.1041781902313232, 1.2502202987670898, 1.250288724899292, 0.8996398448944092, 0.9999365210533142]\n",
      "Saved K8ZSVORQUDTL\n",
      "[1.2464162111282349, 0.7441971898078918, 0.9018120765686035, 0.6540858149528503, 1.1042321920394897, 1.25028657913208, 1.2503538131713867, 0.8996741771697998, 0.9999943375587463]\n",
      "Saved D5BWALVOWDAG\n",
      "[1.2463550567626953, 0.744168758392334, 0.9017560482025146, 0.6540517807006836, 1.1041758060455322, 1.2502214908599854, 1.2502920627593994, 0.8996342420578003, 0.999940037727356]\n",
      "Saved F5QFGHIALRFW\n",
      "[1.2463825941085815, 0.7441856265068054, 0.9017752408981323, 0.6540652513504028, 1.1041998863220215, 1.2502472400665283, 1.2503143548965454, 0.8996506929397583, 0.9999645352363586]\n",
      "Saved C3KSGZMKJDKV\n",
      "[1.2463089227676392, 0.7441515922546387, 0.9017149209976196, 0.6540265083312988, 1.1041334867477417, 1.250176191329956, 1.2502467632293701, 0.8995972871780396, 0.9998988509178162]\n",
      "Saved V5SEWXFPFBFS\n",
      "[1.2465695142745972, 0.744270920753479, 0.901957631111145, 0.6541693210601807, 1.1043598651885986, 1.2504472732543945, 1.2505016326904297, 0.8997784852981567, 1.0001293420791626]\n",
      "Saved J1FTTMAZQTAK\n",
      "[1.2464804649353027, 0.7442284822463989, 0.9018748998641968, 0.6541239023208618, 1.1042859554290771, 1.250354528427124, 1.2504137754440308, 0.8997159004211426, 1.0000499486923218]\n",
      "Saved N4OVAEPKWBFN\n",
      "[1.246260643005371, 0.7441186904907227, 0.9016741514205933, 0.6540004014968872, 1.1040902137756348, 1.2501300573349, 1.2502031326293945, 0.8995697498321533, 0.9998642206192017]\n",
      "Saved C7UQLNBLIXLQ\n",
      "[1.246275544166565, 0.7441178560256958, 0.9016828536987305, 0.6540123820304871, 1.104107141494751, 1.2501401901245117, 1.250216007232666, 0.899579644203186, 0.9998746514320374]\n",
      "Saved A2ODYFNZIRBM\n",
      "[1.2463221549987793, 0.7441585659980774, 0.9017237424850464, 0.6540340185165405, 1.104148507118225, 1.2501871585845947, 1.250258207321167, 0.8996082544326782, 0.9999116659164429]\n",
      "Saved I9OHPEDNKSYS\n",
      "[1.2463579177856445, 0.7441549897193909, 0.9017566442489624, 0.6540542840957642, 1.104180097579956, 1.2502254247665405, 1.2502992153167725, 0.8996376991271973, 0.9999479651451111]\n",
      "Saved H0DAHNXQTIKV\n",
      "[1.2463369369506836, 0.7441554069519043, 0.9017369747161865, 0.6540431976318359, 1.1041572093963623, 1.250196933746338, 1.2502681016921997, 0.899626612663269, 0.9999206066131592]\n",
      "Saved Y7QMGGINLGGC\n",
      "[1.2464046478271484, 0.7441933155059814, 0.9018003940582275, 0.6540780067443848, 1.1042182445526123, 1.250271201133728, 1.2503385543823242, 0.8996680974960327, 0.9999813437461853]\n",
      "Saved G7DLDOOTHQNV\n",
      "[1.2463858127593994, 0.7441849112510681, 0.9017952680587769, 0.6540721654891968, 1.1041995286941528, 1.2502593994140625, 1.2503256797790527, 0.8996551036834717, 0.9999670386314392]\n",
      "Saved U0JPHWOTCKTF\n",
      "[1.246516466140747, 0.7442446947097778, 0.9019030332565308, 0.6541383266448975, 1.1043150424957275, 1.2503902912139893, 1.2504496574401855, 0.8997437953948975, 1.0000848770141602]\n",
      "Saved B6VDHQRVDUZK\n",
      "[1.2464241981506348, 0.7442011833190918, 0.9018139839172363, 0.6540881395339966, 1.1042362451553345, 1.2502893209457397, 1.25035560131073, 0.8996783494949341, 1.0000009536743164]\n",
      "Saved P1YABTHUBXLI\n",
      "[1.246381402015686, 0.7441819906234741, 0.9017833471298218, 0.6540670394897461, 1.1041984558105469, 1.2502493858337402, 1.2503201961517334, 0.8996517658233643, 0.9999629259109497]\n",
      "Saved P8BLXXVFSUEB\n",
      "[1.2464873790740967, 0.7442256212234497, 0.9018880128860474, 0.6541252136230469, 1.1042896509170532, 1.250368356704712, 1.2504299879074097, 0.8997231721878052, 1.0000624656677246]\n",
      "Saved X6IQJRQQRJWS\n",
      "[1.246260643005371, 0.7441246509552002, 0.9016567468643188, 0.653998851776123, 1.1040966510772705, 1.2501170635223389, 1.250194787979126, 0.8995673656463623, 0.99985671043396]\n",
      "Saved N2LQTZGOVDDP\n",
      "[1.246361494064331, 0.7441731691360474, 0.9017444849014282, 0.6540499925613403, 1.104177713394165, 1.2502179145812988, 1.2502856254577637, 0.8996337652206421, 0.999944806098938]\n",
      "Saved Z7DPNMVQSYIY\n",
      "[1.246422290802002, 0.7441942095756531, 0.9018405675888062, 0.6540892720222473, 1.1042317152023315, 1.2503042221069336, 1.250377893447876, 0.8996762037277222, 1.000007152557373]\n",
      "Saved N9DPWNNQMFQS\n",
      "[1.2463679313659668, 0.7441725134849548, 0.9017788171768188, 0.6540637016296387, 1.1041866540908813, 1.2502405643463135, 1.2503104209899902, 0.8996390104293823, 0.9999527931213379]\n",
      "Saved N3MOMNYBSSYD\n",
      "[1.2464196681976318, 0.7441986799240112, 0.9018102884292603, 0.6540880799293518, 1.1042330265045166, 1.2502851486206055, 1.250352382659912, 0.8996745347976685, 0.9999943971633911]\n",
      "Saved O2OGNOYRKTWS\n",
      "[1.2465155124664307, 0.7442449927330017, 0.901907205581665, 0.6541401147842407, 1.1043156385421753, 1.2503927946090698, 1.250450849533081, 0.8997421264648438, 1.000083327293396]\n",
      "Saved S1PHEJYCZMYB\n",
      "[1.2465040683746338, 0.7442333698272705, 0.9019128084182739, 0.6541405916213989, 1.104301929473877, 1.2503857612609863, 1.2504465579986572, 0.8997364044189453, 1.000070571899414]\n",
      "Saved M3JDZYFXHRMZ\n",
      "[1.2463295459747314, 0.7441583871841431, 0.9017249345779419, 0.654036283493042, 1.104154348373413, 1.2501895427703857, 1.2502624988555908, 0.8996162414550781, 0.9999157190322876]\n",
      "Saved S0DHSXOWJOEY\n",
      "[1.2463080883026123, 0.7441398501396179, 0.9017176628112793, 0.6540290117263794, 1.104134440422058, 1.2501771450042725, 1.2502509355545044, 0.8996015787124634, 0.9999043941497803]\n",
      "Saved L1EJTOZTZKKQ\n",
      "[1.2462100982666016, 0.7440946102142334, 0.9016152620315552, 0.6539736986160278, 1.1040525436401367, 1.2500677108764648, 1.250147819519043, 0.8995331525802612, 0.9998158812522888]\n",
      "Saved C6LPFAORSVOF\n",
      "[1.2462937831878662, 0.744132936000824, 0.9016826152801514, 0.6540138721466064, 1.1041243076324463, 1.2501509189605713, 1.2502284049987793, 0.8995895385742188, 0.9998916983604431]\n",
      "Saved D4XVRSPGQHKC\n",
      "[1.2462396621704102, 0.74411541223526, 0.9016293287277222, 0.6539794206619263, 1.104081392288208, 1.250091552734375, 1.2501753568649292, 0.8995494842529297, 0.9998432993888855]\n",
      "Saved L1WQEFFTHSPQ\n",
      "[1.246402382850647, 0.7441871166229248, 0.9018046855926514, 0.654080867767334, 1.104215145111084, 1.2502686977386475, 1.2503392696380615, 0.8996678590774536, 0.9999754428863525]\n",
      "Saved W0QDPWNZKUHD\n",
      "[1.2464404106140137, 0.7442125082015991, 0.9018380641937256, 0.6541042327880859, 1.1042630672454834, 1.2503271102905273, 1.250385046005249, 0.8997001647949219, 1.0000312328338623]\n",
      "Saved M8HCNOCCKLXA\n",
      "[1.2463860511779785, 0.7441823482513428, 0.9017891883850098, 0.6540703773498535, 1.1042038202285767, 1.2502577304840088, 1.2503247261047363, 0.8996541500091553, 0.9999699592590332]\n",
      "Saved A3BYKOUWHLMC\n",
      "[1.2464386224746704, 0.744208037853241, 0.9018220901489258, 0.6540950536727905, 1.1042503118515015, 1.2503042221069336, 1.2503689527511597, 0.8996886014938354, 1.0000154972076416]\n",
      "Saved D1BZXWAWXNHF\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    batch = next(iter(validation_loader))    # might have some issues, see https://github.com/pytorch/pytorch/issues/1917\n",
    "    shirt_dataset_normalized.save_prediction_batch(model(batch['features']), batch['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
