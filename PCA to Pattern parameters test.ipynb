{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducibility when testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see https://pytorch.org/docs/stable/notes/randomness.html\n",
    "# torch.manual_seed(0)\n",
    "\n",
    "# when using cuda\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import scipy.io  # for reading matlab matrix\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom DataSet class\n",
    "class ParametrizedShirtDataSet(Dataset):\n",
    "    \"\"\"\n",
    "    For loading the data of \"Learning Shared Shape Space..\" paper\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir, transform = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the t-shirt examples as subfolders\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "        self.root_path = Path(root_dir)\n",
    "        \n",
    "        # list of items = subfolders\n",
    "        self.datapoints_names = next(os.walk(self.root_path))[1]\n",
    "        \n",
    "        # datapoint folder structure\n",
    "        self.mesh_filename = 'shirt_mesh_r.obj'\n",
    "        self.pattern_params_filename = 'shirt_info.txt'\n",
    "        self.features_filename = 'visfea.mat'\n",
    "        \n",
    "    def update_transform(self, transform):\n",
    "        \"\"\"apply new transform when loading the data\"\"\"\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Number of entries in the dataset\"\"\"\n",
    "        return len(self.datapoints_names)\n",
    "    \n",
    "    \n",
    "    def read_features(self, datapoint_name):\n",
    "        \"\"\"features parameters from a given datapoint subfolder\"\"\"\n",
    "        assert (self.root_path / datapoint_name / self.features_filename).exists()\n",
    "        \n",
    "        matlab_mat = scipy.io.loadmat(self.root_path / datapoint_name / self.features_filename)\n",
    "        # assuming fea1 is what we need\n",
    "        return np.asarray(matlab_mat['fea2']).squeeze()\n",
    "    \n",
    "    \n",
    "    def read_pattern_params(self, datapoint_name):\n",
    "        \"\"\"9 pattern size parameters from a given datapoint subfolder\"\"\"\n",
    "        assert (self.root_path / datapoint_name / self.pattern_params_filename).exists()\n",
    "        \n",
    "        # assuming that we need the numbers from the last line in file\n",
    "        with open(self.root_path / datapoint_name / self.pattern_params_filename) as f:\n",
    "            lines = f.readlines()\n",
    "            params = np.fromstring(lines[-1],  sep = ' ')\n",
    "        return params\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Called when indexing: read the corresponding data. \n",
    "        Does not support list indexing\"\"\"\n",
    "        \n",
    "        if torch.is_tensor(idx):  # allow indexing by tensors\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        datapoint_name = self.datapoints_names[idx]\n",
    "        \n",
    "        features = self.read_features(datapoint_name)\n",
    "        \n",
    "        # read the pattern parameters\n",
    "        pattern_parameters = self.read_pattern_params(datapoint_name)\n",
    "        \n",
    "        sample = {'features' : features, 'pattern_params' : pattern_parameters}\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transforms -- to tensor\n",
    "class SampleToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        features, params = sample['features'], sample['pattern_params']\n",
    "        \n",
    "        return {'features': torch.from_numpy(features).float(), 'pattern_params': torch.from_numpy(params).float()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transforms -- normalize\n",
    "class NormalizeInputfeatures(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "    def __init__(self, mean_features, std_features):\n",
    "        self.mean = mean_features\n",
    "        self.std = std_features\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        features, params = sample['features'], sample['pattern_params']\n",
    "        \n",
    "        return {'features': torch.div((features - self.mean), self.std), 'pattern_params': params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Normalization?\n",
    "\n",
    "def get_mean_std(dataloader):\n",
    "    \n",
    "    stats = { 'batch_sums' : [], 'batch_sq_sums' : []}\n",
    "    \n",
    "    for data in dataloader:\n",
    "        batch_sum = data['features'].sum(0)\n",
    "        stats['batch_sums'].append(batch_sum)\n",
    "\n",
    "    mean_features = sum(stats['batch_sums']) / len(dataloader)\n",
    "    \n",
    "    for data in dataloader:\n",
    "        batch_sum_sq = (data['features'] - mean_features.view(1, len(mean_features)))**2\n",
    "        stats['batch_sq_sums'].append(batch_sum_sq.sum(0))\n",
    "                        \n",
    "    std_features = torch.sqrt(sum(stats['batch_sq_sums']) / len(dataloader))\n",
    "    \n",
    "    return mean_features, std_features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1050\n",
      "torch.Size([100])\n",
      "torch.Size([9])\n",
      "tensor([22.3122, 80.2369, 94.7878, 84.7305, 87.0633, 94.7269, 87.1609, 68.4383,\n",
      "        54.0221, 42.9530, 36.4926, 34.2573, 36.1983, 39.6058, 42.5963, 45.8251,\n",
      "        49.0148, 51.9638, 54.5384, 56.2717, 57.3045, 59.2304, 60.3443, 60.7170,\n",
      "        60.3732, 61.0602, 61.7098, 61.3839, 61.6671, 61.5640, 61.4332, 60.3661,\n",
      "        58.6125, 56.2172, 53.0113, 49.2049, 44.8146, 39.5825, 34.9495, 30.5814,\n",
      "        27.0672, 23.1549, 20.1863, 18.3758, 17.5469, 17.5358, 17.2857, 17.4499,\n",
      "        17.6876, 18.2172, 18.5054, 18.3967, 18.6611, 18.9819, 18.8736, 18.9814,\n",
      "        18.6072, 18.4159, 18.4734, 18.9060, 18.9909, 19.0221, 19.1951, 19.2275,\n",
      "        19.2909, 19.3687, 19.2872, 19.1948, 19.3135, 19.4129, 19.3294, 19.1605,\n",
      "        19.0412, 19.0545, 18.9051, 18.6651, 18.6090, 18.4858, 18.4188, 18.1001,\n",
      "        17.7318, 17.6019, 17.6670, 17.5875, 17.6099, 17.4483, 17.4991, 17.7228,\n",
      "        17.8360, 17.9263, 18.0252, 18.2533, 18.7004, 18.9687, 19.3407, 21.3461,\n",
      "        28.9066, 55.1644, 94.5933, 92.9626]) tensor([ 63.6269, 228.4601, 269.7968, 241.2072, 247.8404, 269.6294, 248.1680,\n",
      "        194.9960, 153.9922, 122.4751, 104.0763,  97.7183, 103.2424, 112.9219,\n",
      "        121.4115, 130.5802, 139.6415, 148.0190, 155.3391, 160.2724, 163.2009,\n",
      "        168.6732, 171.8379, 172.8956, 171.9117, 173.8675, 175.7162, 174.7905,\n",
      "        175.6089, 175.3235, 174.9658, 171.9444, 166.9802, 160.1944, 151.1116,\n",
      "        140.3161, 127.8519, 112.9979,  99.8367,  87.4118,  77.3987,  66.2096,\n",
      "         57.6814,  52.4660,  50.0914,  50.0525,  49.3213,  49.7894,  50.4779,\n",
      "         52.0084,  52.8397,  52.5319,  53.2937,  54.2218,  53.9120,  54.2004,\n",
      "         53.0854,  52.5274,  52.6798,  53.9182,  54.1566,  54.2458,  54.7402,\n",
      "         54.8361,  55.0141,  55.2364,  54.9958,  54.7323,  55.0730,  55.3573,\n",
      "         55.1212,  54.6365,  54.2949,  54.3329,  53.9093,  53.2276,  53.0742,\n",
      "         52.7285,  52.5478,  51.6332,  50.5782,  50.2092,  50.4038,  50.1749,\n",
      "         50.2393,  49.7709,  49.9120,  50.5558,  50.8789,  51.1313,  51.4053,\n",
      "         52.0573,  53.3351,  54.1197,  55.1816,  60.9227,  82.5185, 157.2566,\n",
      "        269.2520, 264.6308])\n"
     ]
    }
   ],
   "source": [
    "# test loading\n",
    "dataset = ParametrizedShirtDataSet(Path('F:\\Learning Shared Shape Space_shirt_dataset_rest'), \n",
    "                                  SampleToTensor())\n",
    "\n",
    "print (len(dataset))\n",
    "print (dataset[1000]['features'].shape)\n",
    "print (dataset[1000]['pattern_params'].shape)\n",
    "#print (dataset[1000])\n",
    "\n",
    "loader = DataLoader(dataset, 10, shuffle=True)\n",
    "\n",
    "mean, std = get_mean_std(loader)\n",
    "print (mean, std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.7280, 10.0000, 10.0000,  5.7167, 10.0000, 10.0000, 10.0000,  1.7942,\n",
      "         2.7499,  3.4947,  4.5604,  1.3809,  1.3401,  1.9693,  2.7231,  4.4794,\n",
      "         5.4335,  4.9750,  6.6098,  7.1386,  6.5891,  6.2242,  5.0481,  6.1127,\n",
      "         6.0178,  5.3848,  6.3810,  5.7453,  5.3307,  3.7287,  8.7910,  9.0789,\n",
      "         9.1678, 10.0000, 10.0000,  2.4115,  0.9612,  1.1466,  1.1053,  1.0611,\n",
      "         1.0211,  1.0210,  0.9864,  0.9422,  2.7900,  2.8249,  1.4483,  1.8033,\n",
      "         1.1050,  0.9412,  1.0041,  0.9973,  0.9732,  0.9511,  0.9558,  1.0157,\n",
      "         1.0167,  0.9513,  0.9543,  1.2004,  1.3300,  0.9807,  0.9578,  0.9972,\n",
      "         1.0205,  0.9093,  0.9328,  0.9820,  0.9938,  0.9847,  1.0011,  1.0261,\n",
      "         0.9751,  1.0542,  1.0290,  1.0384,  1.0466,  1.0319,  1.0156,  1.0526,\n",
      "         1.1265,  1.2638,  1.1140,  1.1654,  1.0753,  1.1316,  1.0372,  1.0565,\n",
      "         1.0130,  1.0182,  0.9955,  1.0444,  1.0865,  1.2461,  1.3008,  1.6466,\n",
      "         2.9357,  7.3296, 10.0000, 10.0000])\n",
      "tensor([-0.3235, -0.3074, -0.3143, -0.3276, -0.3109, -0.3142, -0.3109, -0.3418,\n",
      "        -0.3330, -0.3222, -0.3068, -0.3364, -0.3376, -0.3333, -0.3284, -0.3166,\n",
      "        -0.3121, -0.3175, -0.3085, -0.3066, -0.3108, -0.3143, -0.3218, -0.3158,\n",
      "        -0.3162, -0.3202, -0.3149, -0.3183, -0.3208, -0.3299, -0.3009, -0.2983,\n",
      "        -0.2961, -0.2885, -0.2846, -0.3335, -0.3430, -0.3401, -0.3390, -0.3377,\n",
      "        -0.3365, -0.3343, -0.3329, -0.3323, -0.2946, -0.2939, -0.3211, -0.3143,\n",
      "        -0.3285, -0.3322, -0.3312, -0.3312, -0.3319, -0.3325, -0.3324, -0.3315,\n",
      "        -0.3314, -0.3325, -0.3326, -0.3284, -0.3261, -0.3326, -0.3332, -0.3325,\n",
      "        -0.3321, -0.3342, -0.3337, -0.3328, -0.3326, -0.3329, -0.3325, -0.3319,\n",
      "        -0.3327, -0.3313, -0.3316, -0.3312, -0.3309, -0.3310, -0.3312, -0.3302,\n",
      "        -0.3283, -0.3254, -0.3284, -0.3273, -0.3291, -0.3278, -0.3298, -0.3297,\n",
      "        -0.3306, -0.3307, -0.3313, -0.3306, -0.3302, -0.3275, -0.3269, -0.3234,\n",
      "        -0.3147, -0.3042, -0.3142, -0.3135])\n"
     ]
    }
   ],
   "source": [
    "dataset_normalized = ParametrizedShirtDataSet(Path('F:\\Learning Shared Shape Space_shirt_dataset_rest'), \n",
    "                                  transforms.Compose([\n",
    "                                      SampleToTensor(), \n",
    "                                      NormalizeInputfeatures(mean, std)]))\n",
    "\n",
    "print (dataset[1]['features'])\n",
    "print (dataset_normalized[1]['features'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShirtfeaturesMLP(nn.Module):\n",
    "    \"\"\"MLP for training on shirts dataset. Assumes 100 features parameters used\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # layers definitions\n",
    "        self.sequence = nn.Sequential(\n",
    "            nn.Linear(100, 120), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(120, 80), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(80, 60),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(60, 9)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x_batch):\n",
    "        return self.sequence(x_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShirtPCAMLP(\n",
      "  (sequence): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=120, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=120, out_features=80, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=80, out_features=60, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=60, out_features=9, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = ShirtfeaturesMLP()\n",
    "\n",
    "print (net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Parameters\n",
    "batch_size = 64\n",
    "epochs_num = 100\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data normalization\n",
    "shirt_dataset = ParametrizedShirtDataSet(Path('F:\\Learning Shared Shape Space_shirt_dataset_rest'), \n",
    "                                  SampleToTensor())\n",
    "\n",
    "mean, std = get_mean_std(DataLoader(shirt_dataset, 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split: 945 / 105\n"
     ]
    }
   ],
   "source": [
    "# Data load and split\n",
    "shirt_dataset_normalized = ParametrizedShirtDataSet(Path('F:\\Learning Shared Shape Space_shirt_dataset_rest'), \n",
    "                                  transforms.Compose([\n",
    "                                      SampleToTensor(), \n",
    "                                      NormalizeInputfeatures(mean, std)]))\n",
    "\n",
    "valid_size = (int) (len(shirt_dataset_normalized) / 10)\n",
    "# split is RANDOM. Might affect performance\n",
    "training_set, validation_set = torch.utils.data.random_split(\n",
    "    shirt_dataset_normalized, \n",
    "    (len(shirt_dataset) - valid_size, valid_size))\n",
    "\n",
    "print ('Split: {} / {}'.format(len(training_set), len(validation_set)))\n",
    "\n",
    "training_loader = DataLoader(training_set, batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_set, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop func\n",
    "\n",
    "def fit(model, regression_loss, optimizer, train_loader):\n",
    "    \n",
    "    for epoch in range (epochs_num):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, batch in enumerate(training_loader):\n",
    "            preds = model(batch['features'])\n",
    "            \n",
    "            loss = regression_loss(preds, batch['pattern_params'])\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # logging\n",
    "            if i % 5 == 4:\n",
    "                wb.log({'epoch': epoch, 'loss': loss})\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(\n",
    "                *[(regression_loss(model(batch['features']), batch['pattern_params']), len(batch)) for batch in validation_loader]\n",
    "            )\n",
    "            \n",
    "        valid_loss = np.sum(losses) / np.sum(nums)\n",
    "        print ('Epoch: {}, Validation Loss: {}'.format(epoch, valid_loss))\n",
    "        wb.log({'epoch': epoch, 'valid_loss': valid_loss})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the definitions\n",
    "# model\n",
    "model = ShirtfeaturesMLP()\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "# loss function\n",
    "regression_loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/maria_korosteleva/Test-Garments-Reconstruction\" target=\"_blank\">https://app.wandb.ai/maria_korosteleva/Test-Garments-Reconstruction</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/maria_korosteleva/Test-Garments-Reconstruction/runs/o392ulj1\" target=\"_blank\">https://app.wandb.ai/maria_korosteleva/Test-Garments-Reconstruction/runs/o392ulj1</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[<wandb.wandb_torch.TorchGraph at 0x25662a035f8>]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init Weights&biases run\n",
    "import wandb as wb\n",
    "wb.init(name = \"fea2 + data normalization\", project = 'Test-Garments-Reconstruction')\n",
    "\n",
    "wb.watch(model, log='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Validation Loss: 0.2201310098171234\n",
      "Epoch: 1, Validation Loss: 0.023226015269756317\n",
      "Epoch: 2, Validation Loss: 0.0065571703016757965\n",
      "Epoch: 3, Validation Loss: 0.0063787889666855335\n",
      "Epoch: 4, Validation Loss: 0.0063648647628724575\n",
      "Epoch: 5, Validation Loss: 0.006375469733029604\n",
      "Epoch: 6, Validation Loss: 0.0063785770907998085\n",
      "Epoch: 7, Validation Loss: 0.006376917473971844\n",
      "Epoch: 8, Validation Loss: 0.00639696978032589\n",
      "Epoch: 9, Validation Loss: 0.006376719102263451\n",
      "Epoch: 10, Validation Loss: 0.006384765263646841\n",
      "Epoch: 11, Validation Loss: 0.006378420162945986\n",
      "Epoch: 12, Validation Loss: 0.00637360755354166\n",
      "Epoch: 13, Validation Loss: 0.00637438939884305\n",
      "Epoch: 14, Validation Loss: 0.006384357810020447\n",
      "Epoch: 15, Validation Loss: 0.006370194256305695\n",
      "Epoch: 16, Validation Loss: 0.006389564834535122\n",
      "Epoch: 17, Validation Loss: 0.006381616462022066\n",
      "Epoch: 18, Validation Loss: 0.0063844723626971245\n",
      "Epoch: 19, Validation Loss: 0.0063771880231797695\n",
      "Epoch: 20, Validation Loss: 0.006388981360942125\n",
      "Epoch: 21, Validation Loss: 0.0063946060836315155\n",
      "Epoch: 22, Validation Loss: 0.006376514211297035\n",
      "Epoch: 23, Validation Loss: 0.00637307483702898\n",
      "Epoch: 24, Validation Loss: 0.006373478565365076\n",
      "Epoch: 25, Validation Loss: 0.006383707746863365\n",
      "Epoch: 26, Validation Loss: 0.006384322885423899\n",
      "Epoch: 27, Validation Loss: 0.006378156132996082\n",
      "Epoch: 28, Validation Loss: 0.006389732472598553\n",
      "Epoch: 29, Validation Loss: 0.006373086012899876\n",
      "Epoch: 30, Validation Loss: 0.006382479332387447\n",
      "Epoch: 31, Validation Loss: 0.006369557697325945\n",
      "Epoch: 32, Validation Loss: 0.0063835023902356625\n",
      "Epoch: 33, Validation Loss: 0.006382939405739307\n",
      "Epoch: 34, Validation Loss: 0.00637061707675457\n",
      "Epoch: 35, Validation Loss: 0.006371262017637491\n",
      "Epoch: 36, Validation Loss: 0.006370510905981064\n",
      "Epoch: 37, Validation Loss: 0.006375188939273357\n",
      "Epoch: 38, Validation Loss: 0.006389230489730835\n",
      "Epoch: 39, Validation Loss: 0.006366700399667025\n",
      "Epoch: 40, Validation Loss: 0.006376955192536116\n",
      "Epoch: 41, Validation Loss: 0.00638721976429224\n",
      "Epoch: 42, Validation Loss: 0.006362052634358406\n",
      "Epoch: 43, Validation Loss: 0.006371724419295788\n",
      "Epoch: 44, Validation Loss: 0.006377693265676498\n",
      "Epoch: 45, Validation Loss: 0.0063943034037947655\n",
      "Epoch: 46, Validation Loss: 0.006379126571118832\n",
      "Epoch: 47, Validation Loss: 0.0063840411603450775\n",
      "Epoch: 48, Validation Loss: 0.00638590008020401\n",
      "Epoch: 49, Validation Loss: 0.006382867693901062\n",
      "Epoch: 50, Validation Loss: 0.006375420838594437\n",
      "Epoch: 51, Validation Loss: 0.006371976342052221\n",
      "Epoch: 52, Validation Loss: 0.006384258158504963\n",
      "Epoch: 53, Validation Loss: 0.006381310056895018\n",
      "Epoch: 54, Validation Loss: 0.006375460885465145\n",
      "Epoch: 55, Validation Loss: 0.006387108936905861\n",
      "Epoch: 56, Validation Loss: 0.00637888815253973\n",
      "Epoch: 57, Validation Loss: 0.006386250723153353\n",
      "Epoch: 58, Validation Loss: 0.006390464026480913\n",
      "Epoch: 59, Validation Loss: 0.006390156224370003\n",
      "Epoch: 60, Validation Loss: 0.006377984769642353\n",
      "Epoch: 61, Validation Loss: 0.006384381093084812\n",
      "Epoch: 62, Validation Loss: 0.006399312987923622\n",
      "Epoch: 63, Validation Loss: 0.006383294239640236\n",
      "Epoch: 64, Validation Loss: 0.006400459446012974\n",
      "Epoch: 65, Validation Loss: 0.006405463442206383\n",
      "Epoch: 66, Validation Loss: 0.006387669593095779\n",
      "Epoch: 67, Validation Loss: 0.0063859522342681885\n",
      "Epoch: 68, Validation Loss: 0.006383303552865982\n",
      "Epoch: 69, Validation Loss: 0.006376367062330246\n",
      "Epoch: 70, Validation Loss: 0.006378313526511192\n",
      "Epoch: 71, Validation Loss: 0.006376854609698057\n",
      "Epoch: 72, Validation Loss: 0.006374147720634937\n",
      "Epoch: 73, Validation Loss: 0.006363801192492247\n",
      "Epoch: 74, Validation Loss: 0.006364763714373112\n",
      "Epoch: 75, Validation Loss: 0.006378568708896637\n",
      "Epoch: 76, Validation Loss: 0.006375894881784916\n",
      "Epoch: 77, Validation Loss: 0.006379721686244011\n",
      "Epoch: 78, Validation Loss: 0.006390509195625782\n",
      "Epoch: 79, Validation Loss: 0.006378420628607273\n",
      "Epoch: 80, Validation Loss: 0.006374698597937822\n",
      "Epoch: 81, Validation Loss: 0.006379103288054466\n",
      "Epoch: 82, Validation Loss: 0.0063819969072937965\n",
      "Epoch: 83, Validation Loss: 0.006389821879565716\n",
      "Epoch: 84, Validation Loss: 0.006382953375577927\n",
      "Epoch: 85, Validation Loss: 0.006372765637934208\n",
      "Epoch: 86, Validation Loss: 0.006373598240315914\n",
      "Epoch: 87, Validation Loss: 0.00637078145518899\n",
      "Epoch: 88, Validation Loss: 0.006374146789312363\n",
      "Epoch: 89, Validation Loss: 0.006384764797985554\n",
      "Epoch: 90, Validation Loss: 0.006384814158082008\n",
      "Epoch: 91, Validation Loss: 0.0064061894081532955\n",
      "Epoch: 92, Validation Loss: 0.006384695880115032\n",
      "Epoch: 93, Validation Loss: 0.006377650424838066\n",
      "Epoch: 94, Validation Loss: 0.006400411017239094\n",
      "Epoch: 95, Validation Loss: 0.00637324433773756\n",
      "Epoch: 96, Validation Loss: 0.006387193687260151\n",
      "Epoch: 97, Validation Loss: 0.006376962177455425\n",
      "Epoch: 98, Validation Loss: 0.006399036385118961\n",
      "Epoch: 99, Validation Loss: 0.006370200775563717\n",
      "Finished training\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "\n",
    "fit(model, regression_loss, optimizer, training_loader)\n",
    "\n",
    "print (\"Finished training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.011753877624869347\n"
     ]
    }
   ],
   "source": [
    "# loss on validation set\n",
    "valid_loss = sum([regression_loss(model(batch['features']), batch['pattern_params']) for batch in validation_loader]) \n",
    "\n",
    "print ('Validation loss: {}'.format(valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
